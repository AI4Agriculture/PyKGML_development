{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from io import open\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from scipy.stats import gaussian_kde\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearsonr2(x, y):\n",
    "    \"\"\"\n",
    "    Mimics `scipy.stats.pearsonr`\n",
    "    Arguments\n",
    "    ---------\n",
    "    x : 1D torch.Tensor\n",
    "    y : 1D torch.Tensor\n",
    "    Returns\n",
    "    -------\n",
    "    r_val : float\n",
    "        pearsonr correlation coefficient between x and y\n",
    "\n",
    "    Scipy docs ref:\n",
    "        https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\n",
    "\n",
    "    Scipy code ref:\n",
    "        https://github.com/scipy/scipy/blob/v0.19.0/scipy/stats/stats.py#L2975-L3033\n",
    "    Example:\n",
    "        >>> x = np.random.randn(100)\n",
    "        >>> y = np.random.randn(100)\n",
    "        >>> sp_corr = scipy.stats.pearsonr(x, y)[0]\n",
    "        >>> th_corr = pearsonr(torch.from_numpy(x), torch.from_numpy(y))\n",
    "        >>> np.allclose(sp_corr, th_corr)\n",
    "    \"\"\"\n",
    "    mean_x = torch.mean(x)\n",
    "    mean_y = torch.mean(y)\n",
    "    xm = x.sub(mean_x)\n",
    "    ym = y.sub(mean_y)\n",
    "    r_num = xm.dot(ym)\n",
    "    r_den = torch.norm(xm, 2) * torch.norm(ym, 2)\n",
    "    r_val = r_num / r_den\n",
    "    r2_val = r_val*r_val\n",
    "    return r2_val.to('cpu').numpy()\n",
    "\n",
    "class R2Loss(nn.Module):\n",
    "    #calculate coefficient of determination\n",
    "    def forward(self, y_pred, y):\n",
    "        var_y = torch.var(y, unbiased=False)\n",
    "        return 1.0 - F.mse_loss(y_pred, y, reduction=\"mean\") / var_y\n",
    "\n",
    "import subprocess as sp\n",
    "import os\n",
    "def get_gpu_memory():\n",
    "  _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "\n",
    "  ACCEPTABLE_AVAILABLE_MEMORY = 1024\n",
    "  COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "  memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "  memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "  print(memory_free_values)\n",
    "  return memory_free_values\n",
    "# use N2O model v1--GRU model\n",
    "class N2OGRU(nn.Module):\n",
    "    def __init__(self, ninp, nhid, nlayers, nout, dropout):\n",
    "        super(N2OGRU, self).__init__()\n",
    "        if nlayers > 1:\n",
    "            self.gru = nn.GRU(ninp, nhid,nlayers,dropout=dropout)\n",
    "        else:\n",
    "            self.gru = nn.GRU(ninp, nhid,nlayers)\n",
    "        #self.densor1 = nn.ReLU() #can test other function\n",
    "        self.densor2 = nn.Linear(nhid, nout)\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1 #may change to a small value\n",
    "        self.densor2.bias.data.zero_()\n",
    "        self.densor2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        output, hidden = self.gru(inputs, hidden)\n",
    "        #output = self.densor1(self.drop(output))\n",
    "        #output = torch.exp(self.densor2(self.drop(output))) # add exp\n",
    "        output = self.densor2(self.drop(output)) # add exp\n",
    "        return output, hidden\n",
    "#bsz should be batch size\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
    "    \n",
    "\n",
    "#spin-up: bsz0 is number of year of data_sp you provided for spin up; bsz0=-1 means data_sp=[]\n",
    "#data_sp is the data you provided\n",
    "#return inihidden for simulation period with first year spin-uped\n",
    "def spinup(model,data_sp,cycle,bsz):\n",
    "    inihidden0=model1.init_hidden(bsz)\n",
    "    for c in range(cycle):\n",
    "        output_dummy,inihidden0 = model(data_sp,inihidden0)\n",
    "    return inihidden0\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss\n",
    "#for multi-task learning, sumloss\n",
    "def myloss_mul_sum(output, target,loss_weights):\n",
    "    loss = 0.0\n",
    "    nout=output.size(2)\n",
    "    for i in range(nout):\n",
    "        loss = loss + loss_weights[i]*torch.mean((output[:,:,i] - target[:,:,i])**2)\n",
    "    return loss\n",
    "def scalar_maxmin(X):\n",
    "    return (X - X.min())/(X.max() - X.min()),X.min(),X.max()\n",
    "\n",
    "#generate input combine statini \n",
    "#x should be size of [seq,batch,n_f1], statini be size of [1,batch,n_f2]\n",
    "def load_ini(x,x_ini):\n",
    "    nrep = x.size(0)\n",
    "    x_ini=x_ini[0,:,:].view(1,x_ini.size(1),x_ini.size(2))\n",
    "    return torch.cat((x,x_ini.repeat(nrep,1,1)),2)\n",
    "\n",
    "class Statini_N2OGRU(nn.Module):\n",
    "    #input model variables are for each module\n",
    "    def __init__(self, ninp1, ninp2, nhid, nlayers, nout1, nout2, dropout):\n",
    "        super(Statini_N2OGRU, self).__init__()\n",
    "        if nlayers > 1:\n",
    "            self.gru1 = nn.GRU(ninp1, nhid,nlayers,dropout=dropout)\n",
    "            self.gru2 = nn.GRU(ninp2, nhid,nlayers,dropout=dropout)\n",
    "        else:\n",
    "            self.gru1 = nn.GRU(ninp1, nhid,nlayers)\n",
    "            self.gru2 = nn.GRU(ninp2, nhid,nlayers)\n",
    "        self.densor1 = nn.Linear(nhid, nout1)\n",
    "        self.densor2 = nn.Linear(nhid, nout2)\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1 #may change to a small value\n",
    "        self.densor1.bias.data.zero_()\n",
    "        self.densor1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor2.bias.data.zero_()\n",
    "        self.densor2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, W_inputs, stat_ini, hidden):\n",
    "        inputs = load_ini(W_inputs,stat_ini)\n",
    "        output1, hidden1 = self.gru1(inputs, hidden[0])\n",
    "        output1 = self.densor1(self.drop(output1)) \n",
    "        inputs = torch.cat((W_inputs,output1),2)\n",
    "        output2, hidden2 = self.gru2(inputs, hidden[1])\n",
    "        output2 = self.densor2(self.drop(output2)) \n",
    "        #need to be careful what is the output orders!!!!!!!!!!!!!\n",
    "        output=torch.cat((output2,output1),2)\n",
    "        hidden=(hidden1,hidden2)\n",
    "        return output, hidden\n",
    "#bsz should be batch size\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\\\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "    \n",
    "class Statini_N2OGRU_v2(nn.Module):\n",
    "    #input model variables are for each module\n",
    "    def __init__(self, ninp1, ninp2, nhid, nlayers, nout1, nout2, dropout):\n",
    "        super(Statini_N2OGRU_v2, self).__init__()\n",
    "        if nlayers > 1:\n",
    "            self.gru1 = nn.GRU(ninp1, nhid,nlayers,dropout=dropout)\n",
    "            self.gru2 = nn.GRU(ninp2, nhid,nlayers,dropout=dropout)\n",
    "        else:\n",
    "            self.gru1 = nn.GRU(ninp1, nhid,nlayers)\n",
    "            self.gru2 = nn.GRU(ninp2, nhid,nlayers)\n",
    "        self.densor1 = nn.Linear(nhid, nout1)\n",
    "        self.densor2 = nn.Linear(nhid, nout2)\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1 #may change to a small value\n",
    "        self.densor1.bias.data.zero_()\n",
    "        self.densor1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor2.bias.data.zero_()\n",
    "        self.densor2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, W_inputs, stat_ini,flux_ini, hidden):\n",
    "        inputs = load_ini(W_inputs,stat_ini)\n",
    "        output1, hidden1 = self.gru1(inputs, hidden[0])\n",
    "        output1 = self.densor1(self.drop(output1)) \n",
    "        inputs = torch.cat((W_inputs,output1),2)\n",
    "        inputs = load_ini(inputs,flux_ini)\n",
    "        output2, hidden2 = self.gru2(inputs, hidden[1])\n",
    "        output2 = self.densor2(self.drop(output2)) \n",
    "        #need to be careful what is the output orders!!!!!!!!!!!!!\n",
    "        output=torch.cat((output2,output1),2)\n",
    "        hidden=(hidden1,hidden2)\n",
    "        return output, hidden\n",
    "#bsz should be batch size\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\\\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "    \n",
    "    \n",
    "class Statini_N2OGRU_v3(nn.Module):\n",
    "    #input model variables are for each module\n",
    "    def __init__(self, ninp1, ninp2, nhid1, nhid2, nlayers1, nlayers2, nout1, nout2, dropout):\n",
    "        super(Statini_N2OGRU_v3, self).__init__()\n",
    "        if nlayers1[0] > 1:\n",
    "            self.gru1_1 = nn.GRU(ninp1[0], nhid1[0],nlayers1[0],dropout=dropout)\n",
    "        else:\n",
    "            self.gru1_1 = nn.GRU(ninp1[0], nhid1[0],nlayers1[0])\n",
    "        if nlayers1[1] > 1:\n",
    "            self.gru1_2 = nn.GRU(ninp1[1], nhid1[1],nlayers1[1],dropout=dropout)\n",
    "        else:\n",
    "            self.gru1_2 = nn.GRU(ninp1[1], nhid1[1],nlayers1[1])\n",
    "        if nlayers1[2] > 1:\n",
    "            self.gru1_3 = nn.GRU(ninp1[2], nhid1[2],nlayers1[2],dropout=dropout)\n",
    "        else:\n",
    "            self.gru1_3 = nn.GRU(ninp1[2], nhid1[2],nlayers1[2])\n",
    "        if nlayers1[3] > 1:\n",
    "            self.gru1_4 = nn.GRU(ninp1[3], nhid1[3],nlayers1[3],dropout=dropout)\n",
    "        else:\n",
    "            self.gru1_4 = nn.GRU(ninp1[3], nhid1[3],nlayers1[3])\n",
    "        if nlayers2 > 1:\n",
    "            self.gru2 = nn.GRU(ninp2, nhid2,nlayers2,dropout=dropout)\n",
    "        else:\n",
    "            self.gru2 = nn.GRU(ninp2, nhid2,nlayers2)\n",
    "\n",
    "        self.densor1_1 = nn.Linear(nhid1[0], nout1[0])\n",
    "        self.densor1_2 = nn.Linear(nhid1[1], nout1[1])\n",
    "        self.densor1_3 = nn.Linear(nhid1[2], nout1[2])\n",
    "        self.densor1_4 = nn.Linear(nhid1[3], nout1[3])\n",
    "        self.densor2 = nn.Linear(nhid2, nout2)\n",
    "        self.nhid1 = nhid1\n",
    "        self.nhid2 = nhid2\n",
    "        self.nlayers1 = nlayers1\n",
    "        self.nlayers2 = nlayers2\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1 #may change to a small value\n",
    "        self.densor1_1.bias.data.zero_()\n",
    "        self.densor1_1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor1_2.bias.data.zero_()\n",
    "        self.densor1_2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor1_3.bias.data.zero_()\n",
    "        self.densor1_3.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor1_4.bias.data.zero_()\n",
    "        self.densor1_4.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor2.bias.data.zero_()\n",
    "        self.densor2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, W_inputs, stat_ini,flux_ini, hidden):\n",
    "        #layer 1 for states simulation\n",
    "        inputs = load_ini(W_inputs,stat_ini[0])\n",
    "        output1_1, hidden1_1 = self.gru1_1(inputs, hidden[0][0])\n",
    "        output1_1 = self.densor1_1(self.drop(output1_1))\n",
    "        inputs = load_ini(W_inputs,stat_ini[1])\n",
    "        output1_2, hidden1_2 = self.gru1_2(inputs, hidden[0][1])\n",
    "        output1_2 = self.densor1_2(self.drop(output1_2))\n",
    "        inputs = load_ini(W_inputs,stat_ini[2])\n",
    "        output1_3, hidden1_3 = self.gru1_3(inputs, hidden[0][2])\n",
    "        output1_3 = self.densor1_3(self.drop(output1_3))\n",
    "        inputs = load_ini(W_inputs,stat_ini[3])\n",
    "        output1_4, hidden1_4 = self.gru1_4(inputs, hidden[0][3])\n",
    "        output1_4 = self.densor1_4(self.drop(output1_4))\n",
    "        \n",
    "        inputs = torch.cat((W_inputs,output1_1,output1_2,output1_3,output1_4),2)\n",
    "        inputs = load_ini(inputs,flux_ini)\n",
    "        #layer two for N2O O2 and N2 simulation\n",
    "        output2, hidden2 = self.gru2(inputs, hidden[1])\n",
    "        output2 = self.densor2(self.drop(output2)) \n",
    "        #need to be careful what is the output orders!!!!!!!!!!!!!\n",
    "        output=torch.cat((output2,output1_1,output1_2,output1_3,output1_4),2)\n",
    "        \n",
    "        hidden=((hidden1_1,hidden1_2,hidden1_3,hidden1_4),hidden2)\n",
    "        return output, hidden\n",
    "#bsz should be batch size\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return ((weight.new_zeros(self.nlayers1[0], bsz, self.nhid1[0]),\\\n",
    "                weight.new_zeros(self.nlayers1[1], bsz, self.nhid1[1]),\\\n",
    "                weight.new_zeros(self.nlayers1[2], bsz, self.nhid1[2]),\\\n",
    "                weight.new_zeros(self.nlayers1[3], bsz, self.nhid1[3])),\\\n",
    "                weight.new_zeros(self.nlayers2, bsz, self.nhid2))\n",
    "\n",
    "\n",
    "\n",
    "class Statini_sq_N2OGRU(nn.Module):\n",
    "    #input model variables are for each module\n",
    "    def __init__(self, ninp1, ninp2, nhid, nlayers, nout1, nout2, dropout):\n",
    "        super(Statini_sq_N2OGRU, self).__init__()\n",
    "        if nlayers > 1:\n",
    "            self.gru1 = nn.GRU(ninp1, nhid,nlayers,dropout=dropout)\n",
    "            self.gru2 = nn.GRU(ninp2, nhid,nlayers,dropout=dropout)\n",
    "        else:\n",
    "            self.gru1 = nn.GRU(ninp1, nhid,nlayers)\n",
    "            self.gru2 = nn.GRU(ninp2, nhid,nlayers)\n",
    "        self.densor1 = nn.Linear(nhid, nout1)\n",
    "        self.densor2 = nn.Linear(nhid, nout2)\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1 #may change to a small value\n",
    "        self.densor1.bias.data.zero_()\n",
    "        self.densor1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor2.bias.data.zero_()\n",
    "        self.densor2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, W_inputs, stat_ini_sq, hidden):\n",
    "        inputs = torch.cat((W_inputs,stat_ini_sq),2)\n",
    "        output1, hidden1 = self.gru1(inputs, hidden[0])\n",
    "        output1 = self.densor1(self.drop(output1)) \n",
    "        inputs = torch.cat((W_inputs,output1),2)\n",
    "        output2, hidden2 = self.gru2(inputs, hidden[1])\n",
    "        output2 = self.densor2(self.drop(output2)) \n",
    "        #need to be careful what is the output orders!!!!!!!!!!!!!\n",
    "        output=torch.cat((output2,output1),2)\n",
    "        hidden=(hidden1,hidden2)\n",
    "        return output, hidden\n",
    "#bsz should be batch size\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\\\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "\n",
    "def get_ini(x,ind,nout):\n",
    "    initials=[]\n",
    "    for i in range(len(ind)):\n",
    "        initials.append(x[:,:,ind[i]].view(x.size(0),x.size(1),nout[i]))\n",
    "    return initials\n",
    "\n",
    "def Z_norm(X):\n",
    "    X_mean=X.mean()\n",
    "    X_std=np.std(np.array(X))\n",
    "    return (X-X_mean)/X_std, X_mean, X_std\n",
    "\n",
    "def Z_norm_reverse(X,Xscaler,units_convert):\n",
    "    return (X*Xscaler[1]+Xscaler[0])*units_convert\n",
    "\n",
    "#check whether start time is within the fertilized period\n",
    "def dropout_check(start_t,fntime_ind):\n",
    "    dropout_ind=False\n",
    "    for t in fntime_ind:\n",
    "        if start_t > t-10 and start_t < t+60:\n",
    "            dropout_ind=True\n",
    "    return dropout_ind\n",
    "        \n",
    "#sample data considering dropout and leadtime    \n",
    "def sample_data(X,Y,slw,slw05,totsq,fnfeature_ind):\n",
    "    maxit=int((totsq-slw)/slw05+1)\n",
    "    #find the fertilized time\n",
    "    fntime_ind=np.where(X[:,1,fnfeature_ind].view(-1).to(\"cpu\").numpy()>0)[0]\n",
    "    #get sliding window data with dropout method\n",
    "    for it in range(maxit):\n",
    "        if it==0:\n",
    "            X_new = X[slw05*it:slw05*it+slw,:,:]\n",
    "            Y_new = Y[slw05*it:slw05*it+slw,:,:]\n",
    "        else:\n",
    "            if not dropout_check(slw05*it,fntime_ind):\n",
    "                X_new = torch.cat((X_new,X[slw05*it:slw05*it+slw,:,:]),1)\n",
    "                Y_new = torch.cat((Y_new,Y[slw05*it:slw05*it+slw,:,:]),1)\n",
    "    #get focused data only for fertilized period with random leading time\n",
    "    for t in fntime_ind:\n",
    "        if t != fntime_ind[-1]:\n",
    "            leadtime=np.random.randint(t-60,t-10)\n",
    "            X_new = torch.cat((X_new,X[leadtime:leadtime+slw,:,:]),1)\n",
    "            Y_new = torch.cat((Y_new,Y[leadtime:leadtime+slw,:,:]),1)\n",
    "    return X_new,Y_new\n",
    "\n",
    "#sample data considering dropout and leadtime    \n",
    "def sample_data_FN(X,Y,totsq,fnfeature_ind):\n",
    "    #find the fertilized time\n",
    "    fntime_ind=np.where(X[:,1,fnfeature_ind].view(-1).to(\"cpu\").numpy()>0)[0]\n",
    "    #get focused data only for fertilized period with random leading time\n",
    "    for t in fntime_ind:\n",
    "        if t == fntime_ind[0]:\n",
    "            X_new = X[t-30:t+90,:,:]\n",
    "            Y_new = Y[t-30:t+90,:,:]\n",
    "        else:\n",
    "            X_new = torch.cat((X_new,X[t-30:t+90,:,:]),1)\n",
    "            Y_new = torch.cat((Y_new,Y[t-30:t+90,:,:]),1)\n",
    "    return X_new,Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([6570, 1980, 16]) torch.Size([6570, 1980, 5])\n",
      "['FERTZR_N', 'RADN', 'TMAX_AIR', 'TDIF_AIR', 'HMAX_AIR', 'HDIF_AIR', 'WIND', 'PRECN', 'PDOY', 'PLANTT', 'TBKDS', 'TCSAND', 'TCSILT', 'TPH', 'TCEC', 'TSOC']\n"
     ]
    }
   ],
   "source": [
    "#for load data\n",
    "#prepare input and output\n",
    "start=1\n",
    "end=18\n",
    "Tx=365 #timesteps\n",
    "tyear=end-start+1\n",
    "out_names=['N2O_FLUX','CO2_FLUX', 'WTR_3','NH4_3','NO3_3']\n",
    "units_convert=[-1000.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0] # to similar scale, mg N m-2, gO2 m-2, mg N m-2\n",
    "#out_names='N2O_FLUX'\n",
    "n_out=len(out_names)\n",
    "#'ATM_CO2' constant, AMENDED_C 0, fire n2o,'FIRE_CH4','STG_DEAD, total 25\n",
    "f_names_c=['RESIDUE_C','HUMUS_C','LITTER_C','CO2_FLUX','O2_FLUX','AUTO_RESP','MICRO_C','SURF_RES','CH4_FLUX',\\\n",
    "         'SURF_DOC_FLUX','SUBS_DOC_FLUX','SURF_DIC_FLUX','SUBS_DIC_FLUX','NBP','SOC_1','SOC_3','SOC_5',\\\n",
    "         'H2_FLUX','ECO_HVST_C','ECO_LAI','ECO_GPP','ECO_RA','ECO_NPP','ECO_RH','TTL_DIC']\n",
    "\n",
    "#constant:ACTV_LYR,'SURF_ICE',total 16\n",
    "f_names_w=['ET','RUNOFF','WATER','DISCHG','SNOWPACK','WTR_1','WTR_3','WTR_5','SURF_WTR','ICE_1','ICE_2','ICE_3',\\\n",
    "           'PSI_1','PSI_3','PSI_5','WTR_TBL']\n",
    "\n",
    "\n",
    "#constant:FIRE_N,total 24\n",
    "f_names_n=['RESIDUE_N','HUMUS_N','FERTZR_N','NET_PL_EXCH_N','NH4','NO3','SURF_DON_FLUX','SUBS_DON_FLUX','SURF_DIN_FLUX',\\\n",
    "           'SUBS_DIN_FLUX','N2O_FLUX','NH3_FLUX','N2_FIXN','MICRO_N','NH4_1','NH4_3','NH4_5',\\\n",
    "           'NO3_1','NO3_3','NO3_5','NH4_RES','NO3_RES','ECO_HVST_N','N2_FLUX'] ######### data include the N2O_FLUX!!!!!!!!\n",
    "\n",
    "\n",
    "#constant:,total 19\n",
    "f_names_e=['RADN','TMAX_AIR','TMIN_AIR','HMAX_AIR','HMIN_AIR','WIND','PRECN','TMAX_SOIL_1','TMIN_SOIL_1',\\\n",
    "           'TMAX_SOIL_3','TMIN_SOIL_3','TMAX_SOIL_5','TMIN_SOIL_5','TMAX_LITTER','TMIN_LITTER','ECND_1','ECND_3','ECND_5',\\\n",
    "           'TTL_SALT_DISCHG']\n",
    "\n",
    "#soil property total 15 with variation in new results\n",
    "fp_names=['TSN','FBCU','PDOY','PDS','PDD','DDOY','PLANTT',\\\n",
    "          'LAT','TLB','TBKDS', 'TCSAND', 'TCSILT', 'TPH', 'TCEC', 'TSOC']\n",
    "\n",
    "\n",
    "\n",
    "f_names0=f_names_c+f_names_w+f_names_n+f_names_e+fp_names\n",
    "\n",
    "f_names=['FERTZR_N','RADN','TMAX_AIR','TMIN_AIR','HMAX_AIR','HMIN_AIR','WIND','PRECN']+\\\n",
    "        ['PDOY','PLANTT','TBKDS', 'TCSAND', 'TCSILT', 'TPH', 'TCEC', 'TSOC']\n",
    "\n",
    "#remove_list=['CO2_FLUX','O2_FLUX','AUTO_RESP','CH4_FLUX','SURF_DOC_FLUX','SUBS_DOC_FLUX',\\\n",
    "#             'SURF_DIC_FLUX','SUBS_DIC_FLUX','H2_FLUX','ECO_GPP','ECO_RA','ECO_NPP','ECO_RH','ET',\\\n",
    "#             'RUNOFF','DISCHG','NET_PL_EXCH_N','SURF_DON_FLUX','SUBS_DON_FLUX','SURF_DIN_FLUX',\\\n",
    "#             'SUBS_DIN_FLUX','N2O_FLUX','NH3_FLUX','N2_FIXN','N2_FLUX','TTL_SALT_DISCHG']\n",
    "#remove_list=['CH4_FLUX','N2O_FLUX','NH3_FLUX','N2_FLUX','H2_FLUX']\n",
    "#for c in remove_list:\n",
    "#    f_names.remove(c)\n",
    "\n",
    "\n",
    "n_f0=len(f_names0)\n",
    "n_f=len(f_names)\n",
    "ind=[]\n",
    "for i in range(n_f):\n",
    "    ind.append(f_names0.index(f_names[i]))\n",
    "    \n",
    "#ind=sorted(ind)\n",
    "f_names=[]\n",
    "for i in ind:\n",
    "    f_names.append(f_names0[i])\n",
    "\n",
    "fn_ind=f_names.index('FERTZR_N')\n",
    "print(fn_ind)\n",
    "\n",
    "fln=20 #20 for full 0-300, 15 for 80-240\n",
    "sln=99\n",
    "bsz0=fln*sln\n",
    "X=np.zeros([Tx*tyear,bsz0,n_f0],dtype=np.float32)\n",
    "Y=np.zeros([Tx*tyear,bsz0,n_out],dtype=np.float32)\n",
    "Xscaler=np.zeros([n_f0,2])\n",
    "Yscaler=np.zeros([n_out,2])\n",
    "#load ecosys results\n",
    "basic_path='D:/machinelearning/pgml_progress/mesocosm/'\n",
    "path_load = basic_path+'99points_metrix_scaled1_v9_X_part1.sav'\n",
    "data0=torch.load(path_load)\n",
    "X[:,:,0:45]=data0['InputX']\n",
    "Xscaler[0:45,:]=data0['Xscaler']\n",
    "path_load = basic_path+'99points_metrix_scaled1_v9_X_part2.sav'\n",
    "data0=torch.load(path_load)\n",
    "X[:,:,45:84]=data0['InputX']\n",
    "Xscaler[45:84,:]=data0['Xscaler']\n",
    "#read soil properties:\n",
    "path_load = basic_path+'99points_statv_v3_scaled1.sav'\n",
    "data0=torch.load(path_load)\n",
    "X[:,:,84:n_f0]=data0['Soil_p']\n",
    "Xscaler[84:n_f0,:]=data0['Soil_p_scaler']\n",
    "#use Z-norm to rescale every parameters\n",
    "#Z-norm for Y\n",
    "indout=[]\n",
    "for i in range(n_out):\n",
    "    indout.append(f_names0.index(out_names[i]))\n",
    "Y[:,:,:]=X[:,:,indout]\n",
    "for i in range(n_out):\n",
    "    Y[:,:,i]=(Y[:,:,i]*(Xscaler[indout[i],1]-Xscaler[indout[i],0])+Xscaler[indout[i],0]) # convert back\n",
    "    Y[:,:,i],Yscaler[i,0],Yscaler[i,1]=Z_norm(Y[:,:,i])       \n",
    "#Z-norm for X\n",
    "X=X[:,:,ind]\n",
    "Xscaler=Xscaler[ind,:]\n",
    "for i in range(len(ind)):\n",
    "    X[:,:,i]=(X[:,:,i]*(Xscaler[i,1]-Xscaler[i,0])+Xscaler[i,0]) # convert back\n",
    "    X[:,:,i],Xscaler[i,0],Xscaler[i,1]=Z_norm(X[:,:,i])  \n",
    "\n",
    "Y=torch.from_numpy(Y)\n",
    "X=torch.from_numpy(X)\n",
    "\n",
    "#need to change to new Z_norm\n",
    "W_names=['TMAX_AIR','TMIN_AIR','HMAX_AIR','HMIN_AIR']\n",
    "Diff_names=['TDIF_AIR','HDIF_AIR']\n",
    "W_ind=[]\n",
    "for i in range(len(W_names)):\n",
    "    W_ind.append(f_names.index(W_names[i]))\n",
    "#replace max-min\n",
    "for i in range(len(Diff_names)):\n",
    "    Vmax=X[:,:,W_ind[i*2]]*Xscaler[W_ind[i*2],1]+Xscaler[W_ind[i*2],0]\n",
    "    Vmin=X[:,:,W_ind[i*2+1]]*Xscaler[W_ind[i*2+1],1]+Xscaler[W_ind[i*2+1],0] \n",
    "    X[:,:,W_ind[i*2+1]]=Vmax-Vmin\n",
    "    X[:,:,W_ind[i*2+1]],Xscaler[W_ind[i*2+1],0],Xscaler[W_ind[i*2+1],1]=Z_norm(X[:,:,W_ind[i*2+1]])\n",
    "    f_names[W_ind[i*2+1]]=Diff_names[i]\n",
    "\n",
    "print(X.size(),Y.size())\n",
    "print(f_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([6570, 1980, 16]) 16\n",
      "torch.Size([6570, 1400, 16]) torch.Size([6570, 1400, 5])\n",
      "[4643]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4643]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shuffled_b=torch.randperm(X.size()[1]) # be aware that random may be different every time\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "#X=X[:,shuffled_b,:].to(device)   #test unshuffled site\n",
    "#Y=Y[:,shuffled_b,:].to(device)\n",
    "X=X.to(device)\n",
    "Y=Y.to(device)\n",
    "print(X.size(),n_f)\n",
    "\n",
    "train_n=70\n",
    "val_n=10\n",
    "test_n=19\n",
    "\n",
    "\n",
    "X_train=X[:,0:train_n*fln,:].view(Tx*tyear,train_n*fln,n_f)\n",
    "X_val=X[:,train_n*fln:(train_n+val_n)*fln,:].view(Tx*tyear,val_n*fln,n_f)\n",
    "X_test=X[:,(train_n+val_n)*fln:(train_n+val_n+test_n)*fln,:].view(Tx*tyear,test_n*fln,n_f)\n",
    "Y_train=Y[:,0:train_n*fln,:].view(Tx*tyear,train_n*fln,n_out)\n",
    "Y_val=Y[:,train_n*fln:(train_n+val_n)*fln,:].view(Tx*tyear,val_n*fln,n_out)\n",
    "Y_test=Y[:,(train_n+val_n)*fln:(train_n+val_n+test_n)*fln,:].view(Tx*tyear,test_n*fln,n_out)\n",
    "\n",
    "flux_vars=['N2O_FLUX']\n",
    "flux_ind=[]\n",
    "for i in range(len(flux_vars)):\n",
    "    flux_ind.append(out_names.index(flux_vars[i]))\n",
    "stat_vars=['CO2_FLUX', 'WTR_3','NH4_3','NO3_3']\n",
    "stat_ind=[]\n",
    "for i in range(len(stat_vars)):\n",
    "    stat_ind.append(out_names.index(stat_vars[i]))\n",
    "#loss weights setup\n",
    "loss_weights=[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]   \n",
    "print(X_train.size(), Y_train.size())\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statini_sq_N2OGRU(\n",
      "  (gru1): GRU(20, 64, num_layers=2, dropout=0.2)\n",
      "  (gru2): GRU(20, 64, num_layers=2, dropout=0.2)\n",
      "  (densor1): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (densor2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (drop): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "20\n",
      "torch.Size([192, 64])\n",
      "Model's state_dict:\n",
      "gru1.weight_ih_l0 \t torch.Size([192, 20])\n",
      "gru1.weight_hh_l0 \t torch.Size([192, 64])\n",
      "gru1.bias_ih_l0 \t torch.Size([192])\n",
      "gru1.bias_hh_l0 \t torch.Size([192])\n",
      "gru1.weight_ih_l1 \t torch.Size([192, 64])\n",
      "gru1.weight_hh_l1 \t torch.Size([192, 64])\n",
      "gru1.bias_ih_l1 \t torch.Size([192])\n",
      "gru1.bias_hh_l1 \t torch.Size([192])\n",
      "gru2.weight_ih_l0 \t torch.Size([192, 20])\n",
      "gru2.weight_hh_l0 \t torch.Size([192, 64])\n",
      "gru2.bias_ih_l0 \t torch.Size([192])\n",
      "gru2.bias_hh_l0 \t torch.Size([192])\n",
      "gru2.weight_ih_l1 \t torch.Size([192, 64])\n",
      "gru2.weight_hh_l1 \t torch.Size([192, 64])\n",
      "gru2.bias_ih_l1 \t torch.Size([192])\n",
      "gru2.bias_hh_l1 \t torch.Size([192])\n",
      "densor1.weight \t torch.Size([4, 64])\n",
      "densor1.bias \t torch.Size([4])\n",
      "densor2.weight \t torch.Size([1, 64])\n",
      "densor2.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "n_a=64 #hidden state number\n",
    "n_l=2 #layer of GRU\n",
    "dropout=0.2\n",
    "model_version='n2o_gru_mesotest_v4_exp1.sav'  #####!!!!!!!!!!!!!!!!!!!! change this before training\n",
    "path_save = basic_path+model_version\n",
    "#output 7 in first module and 3 in second module\n",
    "model1=Statini_sq_N2OGRU(n_f+len(stat_ind),n_f+len(stat_ind),n_a,n_l,len(stat_ind),len(flux_ind),dropout)\n",
    "model1.to(device)\n",
    "print(model1)\n",
    "params = list(model1.parameters())\n",
    "print(len(params))\n",
    "print(params[5].size())  # conv1's .weight\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model1.state_dict():\n",
    "    print(param_tensor, \"\\t\", model1.state_dict()[param_tensor].size())\n",
    "loss_val_best = 500000\n",
    "R2_best=0.5\n",
    "compute_r2=R2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training epoch 1\n",
      "train_loss:  11.426259852074958 train_R2 -0.0766153335571289 val_loss: 10.971647262573242 val_R2 -0.05228304862976074 loss val best: 500000 R2 val best: 0.5 Spending time: 1.0159797668457031s\n",
      "finished training epoch 2\n",
      "train_loss:  10.561539414641144 train_R2 -0.028187274932861328 val_loss: 10.210789680480957 val_R2 -0.012430071830749512 loss val best: 500000 R2 val best: 0.5 Spending time: 1.7699642181396484s\n",
      "finished training epoch 3\n",
      "train_loss:  9.79141157942933 train_R2 0.006058335304260254 val_loss: 9.490487098693848 val_R2 0.01800251007080078 loss val best: 500000 R2 val best: 0.5 Spending time: 2.5199592113494873s\n",
      "finished training epoch 4\n",
      "train_loss:  9.14917434964861 train_R2 0.022379636764526367 val_loss: 8.950665473937988 val_R2 0.031219065189361572 loss val best: 500000 R2 val best: 0.5 Spending time: 3.2749407291412354s\n",
      "finished training epoch 5\n",
      "train_loss:  8.68730504791458 train_R2 0.03316015005111694 val_loss: 8.599532127380371 val_R2 0.04045802354812622 loss val best: 500000 R2 val best: 0.5 Spending time: 4.054855823516846s\n",
      "finished training epoch 6\n",
      "train_loss:  8.363038719474496 train_R2 0.04523921012878418 val_loss: 8.319637298583984 val_R2 0.05378139019012451 loss val best: 500000 R2 val best: 0.5 Spending time: 4.818942070007324s\n",
      "finished training epoch 7\n",
      "train_loss:  8.078699947951677 train_R2 0.05940699577331543 val_loss: 8.110815048217773 val_R2 0.06647390127182007 loss val best: 500000 R2 val best: 0.5 Spending time: 5.580904722213745s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-310cd269c22a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train the model and pring loss/ yearly training\n",
    "starttime=time.time()\n",
    "lr=0.1 #sgd\n",
    "lr_adam=0.0001\n",
    "optimizer = optim.Adam(model1.parameters(), lr=lr_adam) #add weight decay normally 1-9e-4\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=600, gamma=0.5)\n",
    "bsz1=train_n*fln\n",
    "bsz_val1=val_n*fln\n",
    "bsz=35*fln\n",
    "bsz_val=10*fln  # this is the batch size for validation, validation not change\n",
    "totsq=Tx*tyear\n",
    "#during training\n",
    "slw=120\n",
    "#sample the training data with sliding window\n",
    "X_train_new, Y_train_new = sample_data_FN(X_train,Y_train,totsq,fn_ind)\n",
    "X_val_new, Y_val_new = sample_data_FN(X_val,Y_val,totsq,fn_ind)\n",
    "#print(X_train_new.size(),X_val_new.size())\n",
    "batch_total=X_train_new.size(1)\n",
    "batch_size=500  # this is the batch size for training\n",
    "#during validation\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "maxepoch=3000\n",
    "model1.train()\n",
    "for epoch in range(maxepoch):\n",
    "    train_loss=0.0\n",
    "    val_loss=0.0\n",
    "    Y_pred_all=torch.zeros(Y_train_new.size(),device=device)\n",
    "    #shuffled the training data\n",
    "    shuffled_b=torch.randperm(X_train_new.size()[1]) \n",
    "    X_train_new=X_train_new[:,shuffled_b,:] \n",
    "    Y_train_new=Y_train_new[:,shuffled_b,:]\n",
    "    model1.zero_grad()\n",
    "    for bb in range(int(batch_total/batch_size)):\n",
    "        \n",
    "        hidden = model1.init_hidden(batch_size)\n",
    "        #generate initial sequence \n",
    "        statini_sq = Y_train_new[0,bb*batch_size:(bb+1)*batch_size,stat_ind].\\\n",
    "                     view(1,batch_size,len(stat_ind)).repeat(X_train_new.size(0),1,1)\n",
    "        #consider midpoints\n",
    "        Y_pred,hidden = model1(X_train_new[:,bb*batch_size:(bb+1)*batch_size,:],\\\n",
    "                                statini_sq,\\\n",
    "                                hidden)\n",
    "        loss = myloss_mul_sum(Y_pred, Y_train_new[:,bb*batch_size:(bb+1)*batch_size,:].view(slw,batch_size,n_out),\\\n",
    "                                 loss_weights)\n",
    "        hidden[0].detach_()\n",
    "        hidden[1].detach_()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            train_loss=train_loss+loss.item()\n",
    "            Y_pred_all[:,bb*batch_size:(bb+1)*batch_size,:]=Y_pred[:,:,:]\n",
    "    scheduler.step()\n",
    "    #validation\n",
    "    model1.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss=train_loss/(batch_total/batch_size)\n",
    "        train_losses.append(train_loss)\n",
    "        #r2 only for n2o\n",
    "        train_R2=compute_r2(Y_pred_all[:,:,0].contiguous().view(-1),Y_train_new[:,:,0].contiguous().view(-1)).item()\n",
    "        Y_val_pred=torch.zeros(Y_val_new.size(),device=device)\n",
    "        #validation for whole 18 years!!!\n",
    "        #generate initial sequence \n",
    "        statini_sq = Y_val_new[0,:,stat_ind].view(1,Y_val_new.size(1),len(stat_ind)).repeat(X_val_new.size(0),1,1)\n",
    "        #change intitial for year round simulation\n",
    "        hidden = model1.init_hidden(X_val_new.size(1))   \n",
    "        Y_val_pred, hidden = model1(X_val_new,statini_sq, hidden)\n",
    "        loss = myloss_mul_sum(Y_val_pred, Y_val_new,loss_weights)\n",
    "        val_loss=loss.item()\n",
    "        val_losses.append(val_loss)\n",
    "        #r2 only for n2o\n",
    "        val_R2=compute_r2(Y_val_pred[:,:,0].contiguous().view(-1),Y_val_new[:,:,0].contiguous().view(-1)).item()\n",
    "        if val_loss < loss_val_best and val_R2 > R2_best:\n",
    "            loss_val_best=val_loss\n",
    "            R2_best = val_R2\n",
    "            f0=open(path_save,'w')\n",
    "            f0.close()\n",
    "            #os.remove(path_save)\n",
    "            torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model1.state_dict(),\n",
    "                    'R2': train_R2,\n",
    "                    'loss': train_loss,\n",
    "                    'los_val': val_loss,\n",
    "                    'R2_val': val_R2,\n",
    "                    }, path_save)    \n",
    "        print(\"finished training epoch\", epoch+1)\n",
    "        mtime=time.time()\n",
    "        print(\"train_loss: \", train_loss, \"train_R2\", train_R2,\"val_loss:\",val_loss,\"val_R2\", val_R2,\\\n",
    "              \"loss val best:\",loss_val_best,\"R2 val best:\",R2_best, f\"Spending time: {mtime - starttime}s\")\n",
    "\n",
    "        if train_R2 > 0.99:\n",
    "            break\n",
    "    model1.train()\n",
    "endtime=time.time()\n",
    "path_fs = path_save+'fs'\n",
    "torch.save({'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'model_state_dict_fs': model1.state_dict(),\n",
    "            }, path_fs)  \n",
    "print(\"final train_loss:\",train_loss,\"final train_R2:\",train_R2,\"val_loss:\",val_loss,\"loss validation best:\",loss_val_best)\n",
    "print(f\"total Training time: {endtime - starttime}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
