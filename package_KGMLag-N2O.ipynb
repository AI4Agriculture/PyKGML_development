{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a465c67a-93c3-4ea9-a9d2-f717797fad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from io import open\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from scipy.stats import gaussian_kde\n",
    "import scipy.stats as stats\n",
    "\n",
    "import subprocess as sp\n",
    "import os\n",
    "def get_gpu_memory():\n",
    "  _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "33574705-c1ab-4a86-9448-65d20a9661c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2Loss(nn.Module):\n",
    "    #calculate coefficient of determination\n",
    "    def forward(self, y_pred, y):\n",
    "        var_y = torch.var(y, unbiased=False)\n",
    "        return 1.0 - F.mse_loss(y_pred, y, reduction=\"mean\") / var_y\n",
    "\n",
    "def myloss_mul_sum(output, target,loss_weights):\n",
    "    loss = 0.0\n",
    "    nout=output.size(2)\n",
    "    for i in range(nout):\n",
    "        loss = loss + loss_weights[i]*torch.mean((output[:,:,i] - target[:,:,i])**2)\n",
    "    return loss\n",
    "    \n",
    "class Statini_sq_N2OGRU(nn.Module):\n",
    "    #input model variables are for each module\n",
    "    def __init__(self, ninp1, ninp2, nhid, nlayers, nout1, nout2, dropout):\n",
    "        super(Statini_sq_N2OGRU, self).__init__()\n",
    "        if nlayers > 1:\n",
    "            self.gru1 = nn.GRU(ninp1, nhid,nlayers,dropout=dropout)\n",
    "            self.gru2 = nn.GRU(ninp2, nhid,nlayers,dropout=dropout)\n",
    "        else:\n",
    "            self.gru1 = nn.GRU(ninp1, nhid,nlayers)\n",
    "            self.gru2 = nn.GRU(ninp2, nhid,nlayers)\n",
    "        self.densor1 = nn.Linear(nhid, nout1)\n",
    "        self.densor2 = nn.Linear(nhid, nout2)\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1 #may change to a small value\n",
    "        self.densor1.bias.data.zero_()\n",
    "        self.densor1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor2.bias.data.zero_()\n",
    "        self.densor2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, W_inputs, stat_ini_sq, hidden):\n",
    "        inputs = torch.cat((W_inputs,stat_ini_sq),2)\n",
    "        output1, hidden1 = self.gru1(inputs, hidden[0])\n",
    "        output1 = self.densor1(self.drop(output1)) \n",
    "        inputs = torch.cat((W_inputs,output1),2)\n",
    "        output2, hidden2 = self.gru2(inputs, hidden[1])\n",
    "        output2 = self.densor2(self.drop(output2)) \n",
    "        #need to be careful what is the output orders!!!!!!!!!!!!!\n",
    "        output=torch.cat((output2,output1),2)\n",
    "        hidden=(hidden1,hidden2)\n",
    "        return output, hidden\n",
    "#bsz should be batch size\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\\\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "\n",
    "\n",
    "class Statini_N2OGRU_v5(nn.Module):\n",
    "    #input model variables are for each module\n",
    "    def __init__(self, ninp1, ninp2, nhid1, nhid2, nlayers1, nlayers2, nout1, nout2, dropout):\n",
    "        super(Statini_N2OGRU_v5, self).__init__()\n",
    "        if nlayers1[0] > 1:\n",
    "            self.gru1_1 = nn.GRU(ninp1[0], nhid1[0],nlayers1[0],dropout=dropout)\n",
    "        else:\n",
    "            self.gru1_1 = nn.GRU(ninp1[0], nhid1[0],nlayers1[0])\n",
    "        if nlayers1[1] > 1:\n",
    "            self.gru1_2 = nn.GRU(ninp1[1], nhid1[1],nlayers1[1],dropout=dropout)\n",
    "        else:\n",
    "            self.gru1_2 = nn.GRU(ninp1[1], nhid1[1],nlayers1[1])\n",
    "        if nlayers1[2] > 1:\n",
    "            self.gru1_3 = nn.GRU(ninp1[2], nhid1[2],nlayers1[2],dropout=dropout)\n",
    "        else:\n",
    "            self.gru1_3 = nn.GRU(ninp1[2], nhid1[2],nlayers1[2])\n",
    "        if nlayers1[3] > 1:\n",
    "            self.gru1_4 = nn.GRU(ninp1[3], nhid1[3],nlayers1[3],dropout=dropout)\n",
    "        else:\n",
    "            self.gru1_4 = nn.GRU(ninp1[3], nhid1[3],nlayers1[3])\n",
    "        if nlayers2 > 1:\n",
    "            self.gru2 = nn.GRU(ninp2, nhid2,nlayers2,dropout=dropout)\n",
    "        else:\n",
    "            self.gru2 = nn.GRU(ninp2, nhid2,nlayers2)\n",
    "\n",
    "        self.densor1_1 = nn.Linear(nhid1[0], nout1[0])\n",
    "        self.densor1_2 = nn.Linear(nhid1[1], nout1[1])\n",
    "        self.densor1_3 = nn.Linear(nhid1[2], nout1[2])\n",
    "        self.densor1_4 = nn.Linear(nhid1[3], nout1[3])\n",
    "        self.densor2 = nn.Linear(nhid2, nout2)\n",
    "        self.nhid1 = nhid1\n",
    "        self.nhid2 = nhid2\n",
    "        self.nlayers1 = nlayers1\n",
    "        self.nlayers2 = nlayers2\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1 #may change to a small value\n",
    "        self.densor1_1.bias.data.zero_()\n",
    "        self.densor1_1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor1_2.bias.data.zero_()\n",
    "        self.densor1_2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor1_3.bias.data.zero_()\n",
    "        self.densor1_3.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor1_4.bias.data.zero_()\n",
    "        self.densor1_4.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor2.bias.data.zero_()\n",
    "        self.densor2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, W_inputs, stat_ini_sq, hidden):\n",
    "        #layer 1 for states simulation\n",
    "        inputs = torch.cat((W_inputs,stat_ini_sq[:,:,0].view(stat_ini_sq.size(0),stat_ini_sq.size(1),1)),2)\n",
    "        output1_1, hidden1_1 = self.gru1_1(inputs, hidden[0][0])\n",
    "        output1_1 = self.densor1_1(self.drop(output1_1))\n",
    "        inputs = torch.cat((W_inputs,stat_ini_sq[:,:,1].view(stat_ini_sq.size(0),stat_ini_sq.size(1),1)),2)\n",
    "        output1_2, hidden1_2 = self.gru1_2(inputs, hidden[0][1])\n",
    "        output1_2 = self.densor1_2(self.drop(output1_2))\n",
    "        inputs = torch.cat((W_inputs,stat_ini_sq[:,:,2].view(stat_ini_sq.size(0),stat_ini_sq.size(1),1)),2)\n",
    "        output1_3, hidden1_3 = self.gru1_3(inputs, hidden[0][2])\n",
    "        output1_3 = self.densor1_3(self.drop(output1_3))\n",
    "        inputs = torch.cat((W_inputs,stat_ini_sq[:,:,3].view(stat_ini_sq.size(0),stat_ini_sq.size(1),1)),2)\n",
    "        output1_4, hidden1_4 = self.gru1_4(inputs, hidden[0][3])\n",
    "        output1_4 = self.densor1_4(self.drop(output1_4))\n",
    "        \n",
    "        inputs = torch.cat((W_inputs,output1_1,output1_2,output1_3,output1_4),2)\n",
    "        #layer two for N2O O2 and N2 simulation\n",
    "        output2, hidden2 = self.gru2(inputs, hidden[1])\n",
    "        output2 = self.densor2(self.drop(output2)) \n",
    "        #need to be careful what is the output orders!!!!!!!!!!!!!\n",
    "        output=torch.cat((output2,output1_1,output1_2,output1_3,output1_4),2)\n",
    "        \n",
    "        hidden=((hidden1_1,hidden1_2,hidden1_3,hidden1_4),hidden2)\n",
    "        return output, hidden\n",
    "#bsz should be batch size\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return ((weight.new_zeros(self.nlayers1[0], bsz, self.nhid1[0]),\\\n",
    "                weight.new_zeros(self.nlayers1[1], bsz, self.nhid1[1]),\\\n",
    "                weight.new_zeros(self.nlayers1[2], bsz, self.nhid1[2]),\\\n",
    "                weight.new_zeros(self.nlayers1[3], bsz, self.nhid1[3])),\\\n",
    "                weight.new_zeros(self.nlayers2, bsz, self.nhid2))\n",
    "        \n",
    "def get_ini(x,ind,nout):\n",
    "    initials=[]\n",
    "    for i in range(len(ind)):\n",
    "        initials.append(x[:,:,ind[i]].view(x.size(0),x.size(1),nout[i]))\n",
    "    return initials\n",
    "\n",
    "def Z_norm(X):\n",
    "    X_mean=X.mean()\n",
    "    X_std=np.std(np.array(X))\n",
    "    return (X-X_mean)/X_std, X_mean, X_std\n",
    "\n",
    "def Z_norm_with_scaler(X,Xscaler):\n",
    "    return (X-Xscaler[0])/Xscaler[1]\n",
    "    \n",
    "def Z_norm_reverse(X,Xscaler,units_convert):\n",
    "    return (X*Xscaler[1]+Xscaler[0])*units_convert\n",
    "\n",
    "#check whether start time is within the fertilized period\n",
    "def dropout_check(start_t,fntime_ind):\n",
    "    dropout_ind=False\n",
    "    for t in fntime_ind:\n",
    "        if start_t > t-10 and start_t < t+60:\n",
    "            dropout_ind=True\n",
    "    return dropout_ind\n",
    "        \n",
    "#sample data considering dropout and leadtime    \n",
    "def sample_data(X,Y,slw,slw05,totsq,fnfeature_ind):\n",
    "    maxit=int((totsq-slw)/slw05+1)\n",
    "    #find the fertilized time\n",
    "    fntime_ind=np.where(X[:,1,fnfeature_ind].view(-1).to(\"cpu\").numpy()>0)[0]\n",
    "    #get sliding window data with dropout method\n",
    "    for it in range(maxit):\n",
    "        if it==0:\n",
    "            X_new = X[slw05*it:slw05*it+slw,:,:]\n",
    "            Y_new = Y[slw05*it:slw05*it+slw,:,:]\n",
    "        else:\n",
    "            if not dropout_check(slw05*it,fntime_ind):\n",
    "                X_new = torch.cat((X_new,X[slw05*it:slw05*it+slw,:,:]),1)\n",
    "                Y_new = torch.cat((Y_new,Y[slw05*it:slw05*it+slw,:,:]),1)\n",
    "    #get focused data only for fertilized period with random leading time\n",
    "    for t in fntime_ind:\n",
    "        if t != fntime_ind[-1]:\n",
    "            leadtime=np.random.randint(t-60,t-10)\n",
    "            X_new = torch.cat((X_new,X[leadtime:leadtime+slw,:,:]),1)\n",
    "            Y_new = torch.cat((Y_new,Y[leadtime:leadtime+slw,:,:]),1)\n",
    "    return X_new,Y_new\n",
    "\n",
    "#sample data considering dropout and leadtime    \n",
    "def sample_data_FN(X, Y, fn_ind):\n",
    "    #find the fertilized time\n",
    "    print(np.sum(X[:,1,fn_ind].to(\"cpu\").numpy()>0))\n",
    "    fntime_ind=np.where(X[:,1,fn_ind].view(-1).to(\"cpu\").numpy()>0)[0]\n",
    "    print(fntime_ind)\n",
    "    #get focused data only for fertilized period with random leading time\n",
    "    for t in fntime_ind:\n",
    "        if t == fntime_ind[0]:\n",
    "            X_new = X[t-30:t+90,:,:]\n",
    "            Y_new = Y[t-30:t+90,:,:]\n",
    "        else:\n",
    "            X_new = torch.cat((X_new,X[t-30:t+90,:,:]),1)\n",
    "            Y_new = torch.cat((Y_new,Y[t-30:t+90,:,:]),1)\n",
    "    return X_new,Y_new\n",
    "def my_loss_weighted(output, target, mask):\n",
    "    loss = torch.mean(((output - target)**2)*mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a8789fa8-8183-4989-8524-1cd8288e7a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create \n",
    "class KGML_N2O_dataset:\n",
    "\n",
    "    def __init__(self,input_path: str, output_path:str, input_data:str) -> None:\n",
    "        '''\n",
    "        data_path: input data directory\n",
    "        input_data: input dataset file name\n",
    "        out_path: output directory\n",
    "        '''\n",
    "        self.data_path = input_path # os.path.abspath(data_path)\n",
    "        self.out_path = output_path # os.path.abspath(out_path)\n",
    "        self.input_data = input_path + input_data\n",
    "        self.fts_names = ['FERTZR_N','RADN','TMAX_AIR','TMIN_AIR','HMAX_AIR','HMIN_AIR','WIND','PRECN'] # 8 weather features\n",
    "        self.fsp_names = ['PDOY', 'PLANTT', 'TBKDS', 'TCSAND', 'TCSILT', 'TPH', 'TCEC', 'TSOC'] # 2 management + 6 soil features\n",
    "        self.f_names = self.fts_names + self.fsp_names # total 16 features\n",
    "        self.out_names = ['N2O_FLUX','CO2_FLUX', 'WTR_3','NH4_3', 'NO3_3'] # target 'N2O_FLUX' and 4 intermediate variables \n",
    "        self.n_f = len(self.f_names) # The number of features\n",
    "        self.n_out = len(self.out_names)\n",
    "        self.n_out1 = self.n_out - 1\n",
    "        self.n_out2 = self.n_out - self.n_out1\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        \n",
    "    def load_data(self):\n",
    "        self.data = torch.tensor(torch.load(self.input_data))[:,:1000,:]\n",
    "        self.X = self.data[:,:,:16]\n",
    "        self.Y = self.data[:,:,16:]\n",
    "        self.batch_total = self.data.size()[1] # site sample size in the 3 dimension data [DOY, site, features]\n",
    "        print(self.data.size())\n",
    "        \n",
    "    def train_test_split(self, train_ratio, test_ratio, val_ratio_to_train):\n",
    "        if train_ratio and not test_ratio:\n",
    "            test_ratio = 1 - train_ratio\n",
    "        if not train_ratio and test_ratio:\n",
    "            train_ratio = 1 - test_ratio\n",
    "        if not val_ratio_to_train:\n",
    "            val_ratio_to_train = 0.1\n",
    "        # shuffle along site-year dimension. This randomization may be different every time\n",
    "        shuffled_ix = torch.randperm(self.X.size()[1])\n",
    "        self.X = self.X[:,shuffled_ix,:].to(self.device)\n",
    "        self.Y = self.Y[:,shuffled_ix,:].to(self.device)\n",
    "        \n",
    "        self.total_n=self.X.size()[1]\n",
    "        self.train_n=int((self.total_n * train_ratio) * (1-val_ratio_to_train))\n",
    "        self.val_n=int((self.total_n * train_ratio) - self.train_n)\n",
    "        self.test_n=self.total_n - self.train_n - self.val_n\n",
    "        \n",
    "        self.X_train=self.X[:,:self.train_n,:]\n",
    "        self.X_val=self.X[:,self.train_n:(self.train_n+self.val_n),:]\n",
    "        self.X_test=self.X[:,(self.train_n+self.val_n):,:]\n",
    "        self.Y_train=self.Y[:,:self.train_n,:]\n",
    "        self.Y_val=self.Y[:,self.train_n:(self.train_n+self.val_n),:]\n",
    "        self.Y_test=self.Y[:,(self.train_n+self.val_n):,:]\n",
    "\n",
    "        #sample the training data with sliding window\n",
    "        self.X_train_new, self.Y_train_new = sample_data_FN(self.X_train, self.Y_train, fn_ind=0)\n",
    "        self.X_val_new, self.Y_val_new = sample_data_FN(self.X_val, self.Y_val, fn_ind=0)\n",
    "        self.X_test_new, self.Y_test_new = sample_data_FN(self.X_test, self.Y_test, fn_ind=0)\n",
    "        # self.X_train_new, self.Y_train_new = self.X_train, self.Y_train\n",
    "        # self.X_val_new, self.Y_val_new = self.X_val, self.Y_val\n",
    "        # self.X_test_new, self.Y_test_new = self.X_test, self.Y_test\n",
    "\n",
    "\n",
    "\n",
    "    # fine tune data augmentation: \n",
    "    # 16 h of data are randomly selected from 24 h observations to compute their mean as the daily value.\n",
    "    # The total number of data is augmented to 122 d × 3 years × 6 chambers × 1000 data samples in this study.\n",
    "    # X_train, Y_train are augmented outputs; X_train_d, Y_train_d are outputs without augmentation.\n",
    "        \n",
    "    def augment_finetune_data(self, dataset_path, scaler_path, val_chamber:int = None, augment_factor: int = None):\n",
    "        scalers = torch.load(scaler_path)\n",
    "        Xscaler = scalers[:16, :]\n",
    "        Yscaler = scalers[16:, :]\n",
    "        if not augment_factor:\n",
    "            augn = 1000\n",
    "        else:\n",
    "            augn = augment_factor\n",
    "        data0=torch.load(dataset_path)\n",
    "        X1=data0['InputX1']\n",
    "        X2=data0['InputX2']\n",
    "        X3=data0['Soil_p']\n",
    "        Y=data0['OutputY']\n",
    "        X1names = ['tair','swdown','precip','spRH'] \n",
    "        X2names = ['Obs_prec','Fertilizer']\n",
    "        X3names=['TSN','FBCU','PDOY','PDS','PDD','DDOY','PLANTT',\\\n",
    "                  'LAT','TLB','TBKDS', 'TCSAND', 'TCSILT', 'TPH', 'TCEC', 'TSOC']\n",
    "        Ynames= ['N2O_FLUX','CO2_FLUX','NO3','NH4','WFPS']\n",
    "        days=122\n",
    "        nyear=3\n",
    "        totnchamber=6\n",
    "        if not val_chamber:\n",
    "            val_chamber = np.random.randint(0, 6)\n",
    "        print(f\"chamber {val_chamber} is used to validate\")\n",
    "        c_index = [0,1,2,3,4,5]\n",
    "        c_val = [val_chamber]\n",
    "        nc_val = len(c_val)\n",
    "        c_train = c_index\n",
    "        c_train.remove(val_chamber)\n",
    "        nc_train = len(c_train)\n",
    "        print(X1.shape,X2.shape,X3.shape,Y.shape)\n",
    "        pred_names=['N2O_FLUX','CO2_FLUX','NO3_3','NH4_3','WTR_3']\n",
    "        #load data n\n",
    "        Ynames_n = [0,1,2,3,4]\n",
    "        #find the pred_names number in out_names, \n",
    "        #the no. of model output Y_train_pred[pred_names_n[i]] will be the related to Y_train[Ynames_n[i]]\n",
    "        pred_names_n = []\n",
    "        for i in range(len(pred_names)):\n",
    "            pred_names_n.append(self.out_names.index(pred_names[i]))\n",
    "            \n",
    "        Y_units_convert=[-24.0,-24.0,1.0,1.0,(1-1.5/2.65)/100.0]\n",
    "    \n",
    "        X_train = np.zeros([days,augn*nyear*nc_train,len(self.f_names)],dtype=np.float32)\n",
    "        Y_train = np.zeros([days,augn*nyear*nc_train,len(Ynames)],dtype=np.float32)\n",
    "        Y_train_mask = np.zeros(Y_train.shape,dtype=np.float32)\n",
    "        #for training without augmentation\n",
    "        X_train_d = np.zeros([days,nyear*nc_train,len(self.f_names)],dtype=np.float32)\n",
    "        Y_train_d = np.zeros([days,nyear*nc_train,len(Ynames)],dtype=np.float32)\n",
    "        Y_train_d_mask = np.zeros(Y_train_d.shape,dtype=np.float32) \n",
    "        \n",
    "        X_val=np.zeros([days,nyear*nc_val,len(self.f_names)],dtype=np.float32)\n",
    "        Y_val=np.zeros([days,nyear*nc_val,len(Ynames)],dtype=np.float32)\n",
    "        Y_val_mask=np.zeros(Y_val.shape,dtype=np.float32) \n",
    "    \n",
    "        \n",
    "        #Y_gt ground truth first day index, for initials creating\n",
    "        Y_train_gt_1stind = np.zeros([nyear*nc_train,len(Ynames)], dtype=int)\n",
    "        Y_val_gt_1stind = np.zeros([nyear*nc_val,len(Ynames)], dtype=int)\n",
    "        print(Y_train_gt_1stind.shape,Y_val_gt_1stind.shape)\n",
    "        #Method: Multidimensional Shifting using NumPy\n",
    "        #method from https://ethankoch.medium.com/incredibly-fast-random-sampling-in-python-baf154bd836a\n",
    "        #product index_array (num_samples,sample_size) within elements\n",
    "        # constants\n",
    "        # returning index\n",
    "        num_samples = augn\n",
    "        sample_size = 16 #sample 16 hours within one day\n",
    "        num_elements = 24\n",
    "        #elements = np.arange(num_elements)\n",
    "        # probabilities should sum to 1\n",
    "        probabilities = np.random.random(num_elements)\n",
    "        probabilities /= np.sum(probabilities)\n",
    "        def multidimensional_shifting(num_samples, sample_size, probabilities):\n",
    "            # replicate probabilities as many times as `num_samples`\n",
    "            replicated_probabilities = np.tile(probabilities, (num_samples, 1))\n",
    "            # get random shifting numbers & scale them correctly\n",
    "            random_shifts = np.random.random(replicated_probabilities.shape)\n",
    "            random_shifts /= random_shifts.sum(axis=1)[:, np.newaxis]\n",
    "            # shift by numbers & find largest (by finding the smallest of the negative)\n",
    "            shifted_probabilities = random_shifts - replicated_probabilities\n",
    "            return np.argpartition(shifted_probabilities, sample_size, axis=1)[:, :sample_size]\n",
    "    \n",
    "        #sample data from mesocosm site chambers\n",
    "        for d in range(days):\n",
    "            #for training data with data augmentation\n",
    "            for y in range(nyear):\n",
    "                for c in range(nc_train):\n",
    "                    #get random sampled indexes\n",
    "                    sample_indexes = multidimensional_shifting(num_samples, sample_size, probabilities)\n",
    "                    #input data\n",
    "                    #temperature\n",
    "                    elements = np.tile(X1[d*24:(d+1)*24,y,c_train[c],0], (num_samples, 1)) # copy the hourly data num_samples times\n",
    "                    output_samples = np.take_along_axis(elements, sample_indexes, axis=1) # sample the data based on random indexes\n",
    "                    output_samples_tmax = output_samples.max(1)\n",
    "                    output_samples_tdif = output_samples_tmax-output_samples.min(1)\n",
    "                    X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),2]=output_samples_tmax\n",
    "                    X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),3]=output_samples_tdif\n",
    "                    X_train_d[d,y*nc_train+c,2] = np.max(X1[d*24:(d+1)*24,y,c_train[c],0])\n",
    "                    X_train_d[d,y*nc_train+c,3] = np.max(X1[d*24:(d+1)*24,y,c_train[c],0])-\\\n",
    "                                                            np.min(X1[d*24:(d+1)*24,y,c_train[c],0])\n",
    "                    #radiation need to convert from W/m-2 to MJ m-2 d-1, *3600*24*10-6\n",
    "                    elements = np.tile(X1[d*24:(d+1)*24,y,c_train[c],1], (num_samples, 1)) # copy the hourly data num_samples times\n",
    "                    output_samples = np.take_along_axis(elements, sample_indexes, axis=1) # sample the data based on random indexes\n",
    "                    output_samples_rad = output_samples.mean(1)*(3600.0*24.0*(10**(-6)))\n",
    "                    X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),1]=output_samples_rad\n",
    "                    X_train_d[d,y*nc_train+c,1] = np.mean(X1[d*24:(d+1)*24,y,c_train[c],1])*(3600.0*24.0*(10**(-6)))\n",
    "                    #humidity\n",
    "                    elements = np.tile(X1[d*24:(d+1)*24,y,c_train[c],3], (num_samples, 1)) # copy the hourly data num_samples times\n",
    "                    output_samples = np.take_along_axis(elements, sample_indexes, axis=1) # sample the data based on random indexes\n",
    "                    output_samples_hmax = output_samples.max(1)\n",
    "                    output_samples_hdif = output_samples_hmax - output_samples.min(1)\n",
    "                    X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),4]=output_samples_hmax\n",
    "                    X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),5]=output_samples_hdif\n",
    "                    X_train_d[d,y*nc_train+c,4] = np.max(X1[d*24:(d+1)*24,y,c_train[c],3])\n",
    "                    X_train_d[d,y*nc_train+c,5] = np.max(X1[d*24:(d+1)*24,y,c_train[c],3]) - \\\n",
    "                                                            np.min(X1[d*24:(d+1)*24,y,c_train[c],3])\n",
    "                    #sample Y data\n",
    "                    for ffy in range(len(Ynames)): \n",
    "                        element=Y[d*24:(d+1)*24,y,c_train[c],ffy]\n",
    "                        nan_nums=np.count_nonzero(np.isnan(element))\n",
    "                        if  nan_nums < 16:\n",
    "                            # copy the hourly data num_samples times\n",
    "                            elements = np.tile(element, (num_samples, 1)) \n",
    "                            # sample the data based on random indexes\n",
    "                            output_samples = np.take_along_axis(elements, sample_indexes, axis=1) \n",
    "                            #convert to right units (n2O g N m-2 h-1 to d-1)\n",
    "                            output_samples_n2o = np.nanmean(output_samples,axis=1)\n",
    "                            # need to be direction to soil\n",
    "                            Y_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),ffy]= output_samples_n2o*Y_units_convert[ffy]\n",
    "                            Y_train_mask[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),ffy] = (24.0-float(nan_nums))/24.0\n",
    "                            \n",
    "                            Y_train_d[d,y*nc_train+c,ffy]= np.nanmean(Y[d*24:(d+1)*24,y,c_train[c],ffy])*\\\n",
    "                                                            Y_units_convert[ffy] #convert \n",
    "                            Y_train_d_mask[d,y*nc_train+c,ffy] = (24.0-float(nan_nums))/24.0\n",
    "                            #get the first day of ground truth\n",
    "                            if Y_train_gt_1stind[y*nc_train+c,ffy] == 0:\n",
    "                                Y_train_gt_1stind[y*nc_train+c,ffy] = d\n",
    "    \n",
    "                        else:\n",
    "                            # if missing value >=16, we use -999 represent nan\n",
    "                            Y_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),ffy]=-999.0 \n",
    "                            Y_train_mask[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),ffy] = 0.0\n",
    "                            \n",
    "                            Y_train_d[d,y*nc_train+c,ffy]=-999.0 \n",
    "                            Y_train_d_mask[d,y*nc_train+c,ffy] = 0.0\n",
    "                    #deal with other training variables\n",
    "                    #fertilizer\n",
    "                    X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),0] = X2[d,y,c_train[c],1]\n",
    "                    X_train_d[d,y*nc_train+c,0] = X2[d,y,c_train[c],1]\n",
    "                    #wind\n",
    "                    X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),6] = 0.05\n",
    "                    X_train_d[d,y*nc_train+c,6] = 0.05\n",
    "                    #precipitation\n",
    "                    X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),7] = X2[d,y,c_train[c],0]\n",
    "                    X_train_d[d,y*nc_train+c,7] = X2[d,y,c_train[c],0]\n",
    "                    for i in range(len(self.fsp_names)):\n",
    "                        X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),8+i] = X3[d,y,c_train[c],X3names.index(self.fsp_names[i])]\n",
    "                        X_train_d[d,y*nc_train+c,8+i] = X3[d,y,c_train[c],X3names.index(self.fsp_names[i])]\n",
    "    \n",
    "    \n",
    "        #load the validation\n",
    "        for d in range(days):\n",
    "            for y in range(nyear):\n",
    "                for c in range(nc_val):\n",
    "                    #temperature\n",
    "                    X_val[d,y*nc_val+c,2] = np.max(X1[d*24:(d+1)*24,y,c_val[c],0])\n",
    "                    X_val[d,y*nc_val+c,3] = np.max(X1[d*24:(d+1)*24,y,c_val[c],0])-\\\n",
    "                                                            np.min(X1[d*24:(d+1)*24,y,c_val[c],0])\n",
    "                    #radiation\n",
    "                    X_val[d,y*nc_val+c,1] = np.mean(X1[d*24:(d+1)*24,y,c_val[c],1])*(3600.0*24.0*(10**(-6)))\n",
    "                    #humidity\n",
    "                    X_val[d,y*nc_val+c,4] = np.max(X1[d*24:(d+1)*24,y,c_val[c],3])\n",
    "                    X_val[d,y*nc_val+c,5] = np.max(X1[d*24:(d+1)*24,y,c_val[c],3]) - \\\n",
    "                                                            np.min(X1[d*24:(d+1)*24,y,c_val[c],3])\n",
    "                    #Y data\n",
    "                    for ffy in range(len(Ynames)): \n",
    "                        element = Y[d*24:(d+1)*24,y,c_val[c],ffy]\n",
    "                        nan_nums=np.count_nonzero(np.isnan(element))\n",
    "                        if  nan_nums < 16:\n",
    "                            Y_val[d,y*nc_val+c,ffy] = np.nanmean(element)*Y_units_convert[ffy] #convert \n",
    "                            Y_val_mask[d,y*nc_val+c,ffy] = (24.0-float(nan_nums))/24.0\n",
    "                            #get the first day of ground truth\n",
    "                            if Y_val_gt_1stind[y*nc_val+c,ffy] == 0:\n",
    "                                Y_val_gt_1stind[y*nc_val+c,ffy] = d\n",
    "                        else:\n",
    "                            Y_val[d,y*nc_val+c,ffy] = -999.0 # if missing value >=16, we use -999 represent nan\n",
    "                            Y_val_mask[d,y*nc_val+c,ffy] = 0.0\n",
    "                    #deal with other training variables\n",
    "                    #fertilizer\n",
    "                    X_val[d,y*nc_val+c,0] = X2[d,y,c_val[c],1]\n",
    "                    #wind\n",
    "                    X_val[d,y*nc_val+c,6] = 0.05\n",
    "                    #precipitation\n",
    "                    X_val[d,y*nc_val+c,7] = X2[d,y,c_val[c],0]\n",
    "                    for i in range(len(self.fsp_names)):\n",
    "                        X_val[d,y*nc_val+c,8+i] = X3[d,y,c_val[c],X3names.index(self.fsp_names[i])]\n",
    "    \n",
    "        print(X_train.shape,Y_train.shape,X_train_d.shape,Y_train_d.shape,X_val.shape,Y_val.shape)\n",
    "        # print(Xscaler.shape, Yscaler.shape,X_train.shape[2])\n",
    "        #Z-norm the matrix\n",
    "        for i in range(X_train.shape[2]):\n",
    "            X_train[:,:,i]=Z_norm_with_scaler(X_train[:,:,i],Xscaler[i,:])\n",
    "            X_train_d[:,:,i]=Z_norm_with_scaler(X_train_d[:,:,i],Xscaler[i,:])\n",
    "            X_val[:,:,i]=Z_norm_with_scaler(X_val[:,:,i],Xscaler[i,:])\n",
    "        for i in range(len(Ynames_n)):\n",
    "            Y_train[:,:,Ynames_n[i]]=Z_norm_with_scaler(Y_train[:,:,Ynames_n[i]],Yscaler[pred_names_n[i],:])\n",
    "            Y_train_d[:,:,Ynames_n[i]]=Z_norm_with_scaler(Y_train_d[:,:,Ynames_n[i]],Yscaler[pred_names_n[i],:])\n",
    "            Y_val[:,:,Ynames_n[i]]=Z_norm_with_scaler(Y_val[:,:,Ynames_n[i]],Yscaler[pred_names_n[i],:])\n",
    "\n",
    "        self.augn = augn\n",
    "        self.c_val = c_val\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_train_d = X_train_d\n",
    "        self.Y_train_d = Y_train_d\n",
    "        self.X_val = X_val\n",
    "        self.Y_val = Y_val\n",
    "        self.Y_train_mask = Y_train_mask\n",
    "        self.Y_val_mask = Y_val_mask\n",
    "        self.Y_train_d_mask = Y_train_d_mask\n",
    "        self.Y_train_gt_1stind = Y_val_gt_1stind\n",
    "        self.Y_val_gt_1stind = Y_val_gt_1stind\n",
    "\n",
    "\n",
    "\n",
    "class KGML_N2O:\n",
    "\n",
    "    def __init__(self,input_path: str, output_path:str, input_data:str, pretrained_model:str, dataset: KGML_N2O_dataset) -> None:\n",
    "        '''\n",
    "        data_path: input data directory\n",
    "        input_data: input dataset file name\n",
    "        out_path: output directory\n",
    "        '''\n",
    "        self.data_path = input_path # os.path.abspath(data_path)\n",
    "        self.out_path = output_path # os.path.abspath(out_path)\n",
    "        self.input_data = input_path + input_data\n",
    "        self.fts_names = ['FERTZR_N','RADN','TMAX_AIR','TMIN_AIR','HMAX_AIR','HMIN_AIR','WIND','PRECN'] # 8 weather features\n",
    "        self.fsp_names = ['PDOY', 'PLANTT', 'TBKDS', 'TCSAND', 'TCSILT', 'TPH', 'TCEC', 'TSOC'] # 2 management + 6 soil features\n",
    "        self.f_names = self.fts_names + self.fsp_names # total 16 features\n",
    "        self.out_names = ['N2O_FLUX','CO2_FLUX', 'WTR_3','NH4_3', 'NO3_3'] # target 'N2O_FLUX' and 4 intermediate variables \n",
    "        self.n_f = len(self.f_names) # The number of features\n",
    "        self.n_out = len(self.out_names)\n",
    "        self.n_out1 = self.n_out - 1\n",
    "        self.n_out2 = self.n_out - self.n_out1\n",
    "        if pretrained_model:\n",
    "            self.pretrained_model = pretrained_model\n",
    "        else:\n",
    "            self.pretrained_model = None\n",
    "        self.dataset = dataset\n",
    "\n",
    "        # if not torch.cuda.is_available():\n",
    "        #     raise RuntimeError(\"GPU is not available. Initialization failed.\")\n",
    "        # Proceed with GPU setup\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        print(\"Using device:\", self.device)\n",
    "    \n",
    "    def load_model(self, drop_out:float = 0.2, n_hidden=64, n_layer=2, seq_len:int = 365, model_version=None):\n",
    "        '''\n",
    "        drop_out # drop out ratio\n",
    "        n_hidden # hidden state number\n",
    "        n_layer # layer of GRU\n",
    "        seq_len # time sequence length\n",
    "        '''  \n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layer = n_layer\n",
    "        self.drop_out = drop_out\n",
    "        \n",
    "        # Format the model version string\n",
    "        current_time = time.localtime()\n",
    "        if not model_version:\n",
    "            model_version = 'n2o_gru_{}-{}-{}.sav'.format(current_time.tm_year, current_time.tm_mon, current_time.tm_mday)\n",
    "\n",
    "        self.model_version = model_version\n",
    "        self.path_save = self.out_path + self.model_version\n",
    "\n",
    "        #output 4 in first module and 1 in second module\n",
    "        self.model = Statini_sq_N2OGRU(self.n_f+self.n_out1, self.n_f+self.n_out1,n_hidden,n_layer,self.n_out1,self.n_out2, drop_out)\n",
    "        if self.pretrained_model:\n",
    "            checkpoint = torch.load(self.pretrained_model, weights_only=False, map_location=torch.device('mps'))\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        print(self.model)\n",
    "        params = list(self.model.parameters())\n",
    "        print(len(params))\n",
    "        print(params[5].size())  # conv1's .weight\n",
    "        print(\"Model's state_dict:\")\n",
    "        for param_tensor in self.model.state_dict():\n",
    "            print(param_tensor, \"\\t\", self.model.state_dict()[param_tensor].size())\n",
    "\n",
    "        \n",
    "    def train(self, LR=0.0001, step_size=600, slw=120, gamma=0.5, batch_size=10, maxepoch=5):\n",
    "\n",
    "        X_train_new, Y_train_new = self.dataset.X_train_new[:, :50, :], self.dataset.Y_train_new[:, :50, :]\n",
    "        X_val_new, Y_val_new = self.dataset.X_val_new[:, :50, :], self.dataset.Y_val_new[:, :50, :]\n",
    "        X_test_new, Y_test_new = self.dataset.X_test_new[:, :50, :], self.dataset.Y_test_new[:, :50, :]\n",
    "        \n",
    "        starttime=time.time()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=LR) #add weight decay normally 1-9e-4\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "        batch_total = X_train_new.size(1)\n",
    "        n_out = self.n_out\n",
    "        compute_r2 = R2Loss()\n",
    "        # loss initials\n",
    "        loss_val_best = 500000\n",
    "        R2_best = 0.5\n",
    "        loss_weights=[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]  \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(maxepoch):\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            Y_pred_all = torch.zeros(Y_train_new.size(),device=self.device)\n",
    "            #shuffled the training data\n",
    "            shuffled_b = torch.randperm(X_train_new.size()[1]) \n",
    "            X_train_new = X_train_new[:,shuffled_b,:] \n",
    "            Y_train_new = Y_train_new[:,shuffled_b,:]\n",
    "            self.model.zero_grad()\n",
    "            for bb in range(int(batch_total/batch_size)):\n",
    "                hidden = self.model.init_hidden(batch_size)\n",
    "                #generate initial sequence \n",
    "                statini_sq = Y_train_new[0,bb*batch_size:(bb+1)*batch_size,-self.n_out1:].\\\n",
    "                             view(1,batch_size,self.n_out1).repeat(X_train_new.size(0),1,1)\n",
    "                #consider midpoints\n",
    "                Y_pred,hidden = self.model(X_train_new[:,bb*batch_size:(bb+1)*batch_size,:],\\\n",
    "                                        statini_sq,\\\n",
    "                                        hidden)\n",
    "                loss = myloss_mul_sum(Y_pred, Y_train_new[:,bb*batch_size:(bb+1)*batch_size,:].view(slw,batch_size,n_out),\\\n",
    "                                         loss_weights)\n",
    "                hidden[0].detach_()\n",
    "                hidden[1].detach_()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                with torch.no_grad():\n",
    "                    train_loss = train_loss+loss.item()\n",
    "                    Y_pred_all[:,bb*batch_size:(bb+1)*batch_size,:]=Y_pred[:,:,:]\n",
    "            scheduler.step()\n",
    "            #validation\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_loss = train_loss/(batch_total/batch_size)\n",
    "                train_losses.append(train_loss)\n",
    "                #r2 only for n2o\n",
    "                train_R2 = compute_r2(Y_pred_all[:,:,0].contiguous().view(-1),Y_train_new[:,:,0].contiguous().view(-1)).item()\n",
    "                Y_val_pred = torch.zeros(Y_val_new.size(),device=self.device)\n",
    "                #generate initial sequence\n",
    "                statini_sq = Y_val_new[0,:,-self.n_out1:].view(1,Y_val_new.size(1),self.n_out1).repeat(X_val_new.size(0),1,1)\n",
    "                #change intitial for year round simulation\n",
    "                hidden = self.model.init_hidden(X_val_new.size(1))   \n",
    "                Y_val_pred, hidden = self.model(X_val_new,statini_sq, hidden)\n",
    "                loss = myloss_mul_sum(Y_val_pred, Y_val_new,loss_weights)\n",
    "                val_loss = loss.item()\n",
    "                val_losses.append(val_loss)\n",
    "                #r2 only for n2o\n",
    "                val_R2 = compute_r2(Y_val_pred[:,:,0].contiguous().view(-1),Y_val_new[:,:,0].contiguous().view(-1)).item()\n",
    "                if val_loss < loss_val_best and val_R2 > R2_best:\n",
    "                    loss_val_best=val_loss\n",
    "                    R2_best = val_R2\n",
    "                    f0 = open(self.path_save,'w')\n",
    "                    f0.close()\n",
    "                    #os.remove(self.path_save)\n",
    "                    torch.save({'epoch': epoch,\n",
    "                            'model_state_dict': self.model.state_dict(),\n",
    "                            'R2': train_R2,\n",
    "                            'loss': train_loss,\n",
    "                            'los_val': val_loss,\n",
    "                            'R2_val': val_R2,\n",
    "                            }, self.path_save)    \n",
    "                print(\"finished training epoch\", epoch+1)\n",
    "                mtime = time.time()\n",
    "                print(\"train_loss: \", train_loss, \"train_R2\", train_R2,\"val_loss:\",val_loss,\"val_R2\", val_R2,\\\n",
    "                      \"loss val best:\",loss_val_best,\"R2 val best:\",R2_best, f\"Spending time: {mtime - starttime}s\")\n",
    "        \n",
    "                if train_R2 > 0.99:\n",
    "                    break\n",
    "            self.model.train()\n",
    "        endtime = time.time()\n",
    "        path_fs = self.path_save+'fs'\n",
    "        torch.save({'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    }, path_fs)  \n",
    "        print(\"final train_loss:\",train_loss,\"final train_R2:\",train_R2,\"val_loss:\",val_loss,\"loss validation best:\",loss_val_best)\n",
    "        print(f\"total Training time: {endtime - starttime}s\")\n",
    "\n",
    "\n",
    "    def test(self, model_version, batch_size=500):\n",
    "        starttime = time.time()\n",
    "        X_test_new, Y_test_new = self.dataset.X_test_new, self.dataset.Y_test_new\n",
    "        compute_r2 = R2Loss()\n",
    "        \n",
    "        ####read in a pretrained model with full sample\n",
    "        if not model_version:\n",
    "            model_version = self.model_version\n",
    "        path_save = self.out_path + model_version\n",
    "        batch_total = X_test_new.size(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            checkpoint = torch.load(path_save, map_location = self.device, weights_only=False)\n",
    "            model_trained = Statini_sq_N2OGRU(self.n_f+self.n_out1, self.n_f+self.n_out1, self.n_hidden, self.n_layer, self.n_out1,self.n_out2, self.drop_out)\n",
    "            model_trained.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model_trained.to(self.device) #too large for GPU, kif not enough, change to cpu\n",
    "            model_trained.eval()\n",
    "            epoch = checkpoint['epoch']\n",
    "            # print(epoch)\n",
    "\n",
    "            # Y_pred = torch.zeros(Y_test_new.size(),device=self.device)\n",
    "            # for bb in range(int(batch_total/batch_size)):\n",
    "            #     if bb != int(batch_total/batch_size)-1:\n",
    "            #         sbb = bb*batch_size\n",
    "            #         ebb = (bb+1)*batch_size\n",
    "            #     else:\n",
    "            #         sbb = bb*batch_size\n",
    "            #         ebb = batch_total\n",
    "                        #model initials\n",
    "            with torch.no_grad():\n",
    "                statini_sq = Y_test_new[0,:,-self.n_out1:].view(1,Y_test_new.size(1),self.n_out1).repeat(Y_test_new.size(0),1,1)\n",
    "                #change intitial for year round simulation\n",
    "                hidden = self.model.init_hidden(X_test_new.size(1))   \n",
    "                Y_pred, hidden = self.model(X_test_new,statini_sq, hidden)\n",
    "                # Y_pred[:,sbb:ebb,:] = Y_pred_t[:,:,:]\n",
    "            self.Y_test_pred = Y_pred\n",
    "            endtime = time.time()\n",
    "            print(f\"Total spending time: {endtime - starttime}s\")\n",
    "\n",
    "\n",
    "\n",
    "    def generate_ini_stats(self, stat_values,Y_gt,Y_gt_1stind,aug_n,pred_names_n,Ynames_n):\n",
    "        statini_sq = torch.zeros([Y_gt.size(0),Y_gt.size(1),len(stat_values)],device=self.device)\n",
    "        for i in range(len(stat_values)):\n",
    "            statini_sq[:,:,i]=stat_values[i]\n",
    "        for i in range(len(pred_names_n)):\n",
    "            for cc in range(len(Y_gt_1stind[:,Ynames_n[i]])):\n",
    "                if Y_gt_1stind[cc,Ynames_n[i]] != 0:\n",
    "                    statini_sq[Y_gt_1stind[cc,Ynames_n[i]]:,cc*aug_n:(cc+1)*aug_n,pred_names_n[i]-1]= \\\n",
    "                                                        Y_gt[Y_gt_1stind[cc,Ynames_n[i]],cc*aug_n:(cc+1)*aug_n,Ynames_n[i]].\\\n",
    "                                                        view(1,aug_n).repeat(Y_gt.size(0)-Y_gt_1stind[cc,Ynames_n[i]],1)\n",
    "        return statini_sq\n",
    "\n",
    "    \n",
    "    def fine_tune(self, basic_path, model_load: str = 'n2o_gru_mesotest_v4_exp1.sav', maxepoch_l1 = 20000, maxepoch_l2 = 800, lr_adam = 0.0001*0.5, slw = 122, step_size=5000, gamma=0.5):    \n",
    "        X1names = ['tair','swdown','precip','spRH'] \n",
    "        X2names = ['Obs_prec','Fertilizer']\n",
    "        X3names=['TSN','FBCU','PDOY','PDS','PDD','DDOY','PLANTT',\\\n",
    "                  'LAT','TLB','TBKDS', 'TCSAND', 'TCSILT', 'TPH', 'TCEC', 'TSOC']\n",
    "        Ynames= ['N2O_FLUX','CO2_FLUX','NO3','NH4','WFPS']\n",
    "        pred_names=['N2O_FLUX','CO2_FLUX','NO3_3','NH4_3','WTR_3']\n",
    "        Ynames_n = [0,1,2,3,4]\n",
    "        stat_values=[-1.0,0.2,0.0,20.0]\n",
    "        pred_names_n = []\n",
    "        for i in range(len(pred_names)):\n",
    "            pred_names_n.append(self.out_names.index(pred_names[i]))\n",
    "\n",
    "        # declare split data and masks from KGML_N2O_dataset\n",
    "        device = self.device\n",
    "        augn = self.dataset.augn\n",
    "        X_train = torch.from_numpy(self.dataset.X_train).to(device)\n",
    "        Y_train = torch.from_numpy(self.dataset.Y_train).to(device)\n",
    "        X_train_d = torch.from_numpy(self.dataset.X_train_d).to(device)\n",
    "        Y_train_d = torch.from_numpy(self.dataset.Y_train_d).to(device)\n",
    "        X_val = torch.from_numpy(self.dataset.X_val).to(device)\n",
    "        Y_val = torch.from_numpy(self.dataset.Y_val).to(device)\n",
    "        Y_train_mask = torch.from_numpy(self.dataset.Y_train_mask).to(device)\n",
    "        Y_val_mask = torch.from_numpy(self.dataset.Y_val_mask).to(device)\n",
    "        Y_train_d_mask = torch.from_numpy(self.dataset.Y_train_d_mask).to(device)\n",
    "        Y_train_gt_1stind = torch.from_numpy(self.dataset.Y_train_gt_1stind).to(device)\n",
    "        Y_val_gt_1stind = torch.from_numpy(self.dataset.Y_val_gt_1stind).to(device)\n",
    "        \n",
    "        Y_train_maskb = Y_train_mask.ge(0.25)\n",
    "        Y_val_maskb = Y_val_mask.ge(0.25)\n",
    "        Y_train_d_maskb = Y_train_d_mask.ge(0.25)\n",
    "        #retrain the model\n",
    "        #########################prepare the fake initials\n",
    "        #Generate initials considering the ground truth:\n",
    "        #Y_gt is the ground truth, using for validate or train\n",
    "    \n",
    "        \n",
    "        #generate initials\n",
    "        \n",
    "        stats_train_sq = self.generate_ini_stats(stat_values, Y_train,\\\n",
    "                                            Y_train_gt_1stind, augn, pred_names_n[1:],Ynames_n[1:])\n",
    "        stats_val_sq = self.generate_ini_stats(stat_values,Y_val,\\\n",
    "                                                    Y_val_gt_1stind,1,pred_names_n[1:],Ynames_n[1:])\n",
    "        \n",
    "        #generate initials for training without \n",
    "        stats_train_d_sq = self.generate_ini_stats(stat_values,Y_train_d,\\\n",
    "                                            Y_train_gt_1stind,1,pred_names_n[1:],Ynames_n[1:])\n",
    "    \n",
    "        def generate_fake_stats(stat_values,batch):\n",
    "            fake_stats=torch.zeros([1,batch,len(stat_values)],device=device)\n",
    "            for i in range(len(stat_values)):\n",
    "                fake_stats[:,:,i]=stat_values[i]\n",
    "            return fake_stats\n",
    "        \n",
    "        \n",
    "        \n",
    "        ################train the model start\n",
    "        model_version=\"n2o_gru_mesotest_val\"+str(self.dataset.c_val[0]+1)+\".sav\"\n",
    "        starttime = time.time()\n",
    "        ####################freeze the second layer, only train the first layer\n",
    "        #only train first layer and states variables and without augmentation\n",
    "        ###load model\n",
    "        path_load = basic_path+model_load\n",
    "        checkpoint = torch.load(path_load, map_location = self.device, weights_only=False)\n",
    "        model1 = Statini_sq_N2OGRU(self.n_f+self.n_out1, self.n_f+self.n_out1, self.n_hidden, self.n_layer, self.n_out1,self.n_out2, self.drop_out)\n",
    "        model1.load_state_dict(checkpoint['model_state_dict'])\n",
    "                \n",
    "        model1.to(device) #too large for GPU, kif not enough, change to cpu\n",
    "        print(model1)\n",
    "        loss_val_best = 500000\n",
    "        R2_best = 0.2\n",
    "        compute_r2 = R2Loss()\n",
    "        path_save = basic_path+model_version+'1l'\n",
    "        \n",
    "        ##freeze 2nd layer\n",
    "        for name, param in model1.named_parameters():\n",
    "            if param.requires_grad and ('gru2' in name or 'densor2' in name):\n",
    "                param.requires_grad = False\n",
    "            if param.requires_grad:\n",
    "                print(name)\n",
    "                \n",
    "        \n",
    "        optimizer = optim.Adam(model1.parameters(), lr=lr_adam) #add weight decay normally 1-9e-4\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "        # print(X_train_d.size(),X_val.size())\n",
    "        batch_total = X_train_d.size(1)\n",
    "        batch_size = batch_total  # this is the batch size for training\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        model1.train()\n",
    "        \n",
    "        for epoch in range(maxepoch_l1):\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            Y_pred_all = torch.zeros([Y_train_d.size(0), Y_train_d.size(1),len(self.out_names)],device=device)\n",
    "            #shuffled the training data\n",
    "            shuffled_b = torch.randperm( X_train_d.size()[1]) \n",
    "            X_train_new =  X_train_d[:,shuffled_b,:]\n",
    "            Y_train_new =  Y_train_d[:,shuffled_b,:]\n",
    "            Y_train_mask_new =  Y_train_d_mask[:,shuffled_b,:]\n",
    "            Y_train_maskb_new =  Y_train_d_maskb[:,shuffled_b,:]\n",
    "            stats_train_sq_new = stats_train_d_sq[:,shuffled_b,:]  \n",
    "            model1.zero_grad()\n",
    "            for bb in range(int(batch_total/batch_size)):\n",
    "                hidden = model1.init_hidden(batch_size)\n",
    "                Y_pred,hidden = model1(X_train_new[:,bb*batch_size:(bb+1)*batch_size,:],\\\n",
    "                                       stats_train_sq_new[:,bb*batch_size:(bb+1)*batch_size,:],hidden)\n",
    "                #need to adjust the loss for missing data based on mask\n",
    "                loss = my_loss_weighted(Y_pred[:,:,pred_names_n[1:]], \\\n",
    "                                        Y_train_new[:,bb*batch_size:(bb+1)*batch_size,Ynames_n[1:]],\\\n",
    "                                        Y_train_mask_new[:,bb*batch_size:(bb+1)*batch_size,Ynames_n[1:]])\n",
    "                hidden[0].detach_() \n",
    "                hidden[1].detach_()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                with torch.no_grad():\n",
    "                    train_loss=train_loss+loss.item()\n",
    "                    Y_pred_all[:,bb*batch_size:(bb+1)*batch_size,:]=Y_pred[:,:,:]\n",
    "           \n",
    "        \n",
    "            scheduler.step()\n",
    "            #validation\n",
    "            model1.eval()\n",
    "            with torch.no_grad():\n",
    "                train_loss=train_loss/(batch_total/batch_size)\n",
    "                train_losses.append(train_loss)\n",
    "                #mask out the points\n",
    "                Y_train_new_masked = torch.masked_select(Y_train_new[:,:,Ynames_n[1:]], Y_train_maskb_new[:,:,Ynames_n[1:]])\n",
    "                Y_pred_all_masked = torch.masked_select(Y_pred_all[:,:,pred_names_n[1:]], Y_train_maskb_new[:,:,Ynames_n[1:]])\n",
    "                train_R2 = compute_r2(Y_pred_all_masked.contiguous().view(-1),\\\n",
    "                                    Y_train_new_masked.contiguous().view(-1)).item()\n",
    "                ########################validation\n",
    "                Y_val_pred = torch.zeros([Y_val.size(0),Y_val.size(1),len(self.out_names)],device=device)\n",
    "                hidden = model1.init_hidden(X_val.size(1))\n",
    "                #print(stats_val_sq[:,2,0])\n",
    "                Y_val_pt, hidden = model1(X_val,stats_val_sq,hidden)\n",
    "                Y_val_pred[:,:,:] = Y_val_pt[:,:,:]\n",
    "                loss = my_loss_weighted(Y_val_pred[:,:,pred_names_n[1:]],Y_val[:,:,Ynames_n[1:]],Y_val_mask[:,:,Ynames_n[1:]])\n",
    "                val_loss=loss.item()\n",
    "                val_losses.append(val_loss)\n",
    "                Y_val_masked=torch.masked_select(Y_val[:,:,Ynames_n[1:]], Y_val_maskb[:,:,Ynames_n[1:]])\n",
    "                Y_val_pred_masked=torch.masked_select(Y_val_pred[:,:,pred_names_n[1:]], Y_val_maskb[:,:,Ynames_n[1:]])\n",
    "                val_R2=compute_r2(Y_val_pred_masked.contiguous().view(-1),Y_val_masked.contiguous().view(-1)).item()\n",
    "                if val_loss < loss_val_best and val_R2 > R2_best:\n",
    "                    loss_val_best=val_loss\n",
    "                    R2_best = val_R2\n",
    "                    f0=open(path_save,'w')\n",
    "                    f0.close()\n",
    "                    #os.remove(path_save)\n",
    "                    torch.save({'epoch': epoch,\n",
    "                            'model_state_dict': model1.state_dict(),\n",
    "                            'R2': train_R2,\n",
    "                            'loss': train_loss,\n",
    "                            'los_val': val_loss,\n",
    "                            'R2_val': val_R2,\n",
    "                            }, path_save)\n",
    "                if epoch%1000 == 999:\n",
    "                    print(\"finished training 1st layer epoch\", epoch+1)\n",
    "                    mtime=time.time()\n",
    "                    print(\"train_loss: \", train_loss, \"train_R2\", train_R2,\"val_loss:\",val_loss,\"val_R2\", val_R2,\\\n",
    "                          \"loss val best:\",loss_val_best,\"R2 val best:\",R2_best, f\"Spending time: {mtime - starttime}s\")\n",
    "                if train_R2 > 0.99:\n",
    "                    break\n",
    "                #adding early stop\n",
    "            model1.train()\n",
    "    \n",
    "        path_fs = path_save+'fs'\n",
    "        torch.save({'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'model_state_dict_fs': model1.state_dict(),\n",
    "                    }, path_fs)\n",
    "        #####finished training first layer\n",
    "        \n",
    "        ####################train the second layer, freeze first layer\n",
    "        \n",
    "        ###load the trained model:\n",
    "        path_load = basic_path+model_version+'1l'\n",
    "        print(path_load)\n",
    "        checkpoint = torch.load(path_load, map_location = self.device, weights_only=False)\n",
    "        model1 = Statini_sq_N2OGRU(self.n_f+self.n_out1, self.n_f+self.n_out1, self.n_hidden, self.n_layer, self.n_out1,self.n_out2, self.drop_out)\n",
    "        model1.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model1.to(device) #too large for GPU, kif not enough, change to cpu\n",
    "        print(model1)\n",
    "        loss_val_best = 500000\n",
    "        R2_best=0.2\n",
    "        path_save = basic_path+model_version+'2l'\n",
    "        \n",
    "        for name, param in model1.named_parameters():\n",
    "            param.requires_grad = True\n",
    "        ###freeze 1st layer\n",
    "        for name, param in model1.named_parameters():\n",
    "            if param.requires_grad and ('gru1' in name or 'densor1' in name):\n",
    "                param.requires_grad = False\n",
    "            if param.requires_grad:\n",
    "                print(name)\n",
    "    \n",
    "    \n",
    "        optimizer = optim.Adam(model1.parameters(), lr=lr_adam) #add weight decay normally 1-9e-4\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "        print(X_train.size(), X_val.size())\n",
    "        batch_total=X_train_d.size(1)\n",
    "        batch_size=batch_total  # this is the batch size for training\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        model1.train()\n",
    "        for epoch in range(maxepoch_l2):\n",
    "            train_loss=0.0\n",
    "            val_loss=0.0\n",
    "            Y_pred_all = torch.zeros([Y_train_d.size(0),Y_train_d.size(1),len(self.out_names)],device=device)\n",
    "            #shuffled the training data\n",
    "            shuffled_b = torch.randperm(X_train_d.size()[1]) \n",
    "            X_train_new = X_train_d[:,shuffled_b,:] \n",
    "            Y_train_new = Y_train_d[:,shuffled_b,:]\n",
    "            Y_train_mask_new = Y_train_d_mask[:,shuffled_b,:]\n",
    "            Y_train_maskb_new = Y_train_d_maskb[:,shuffled_b,:]\n",
    "            stats_train_sq_new = stats_train_d_sq[:,shuffled_b,:]  \n",
    "            model1.zero_grad()\n",
    "            for bb in range(int(batch_total/batch_size)):\n",
    "                hidden = model1.init_hidden(batch_size)\n",
    "                \n",
    "                Y_pred,hidden = model1(X_train_new[:,bb*batch_size:(bb+1)*batch_size,:],\\\n",
    "                                       stats_train_sq_new[:,bb*batch_size:(bb+1)*batch_size,:],hidden)\n",
    "                #need to adjust the loss for missing data based on mask\n",
    "                loss = my_loss_weighted(Y_pred[:,:,pred_names_n[0]], \\\n",
    "                                        Y_train_new[:,bb*batch_size:(bb+1)*batch_size,Ynames_n[0]],\\\n",
    "                                        Y_train_mask_new[:,bb*batch_size:(bb+1)*batch_size,Ynames_n[0]])\n",
    "                hidden[0].detach_() \n",
    "                hidden[1].detach_()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                with torch.no_grad():\n",
    "                    train_loss=train_loss+loss.item()\n",
    "                    Y_pred_all[:,bb*batch_size:(bb+1)*batch_size,:]=Y_pred[:,:,:]\n",
    "    \n",
    "    \n",
    "            scheduler.step()\n",
    "            #validation\n",
    "            model1.eval()\n",
    "            with torch.no_grad():\n",
    "                train_loss=train_loss/(batch_total/batch_size)\n",
    "                train_losses.append(train_loss)\n",
    "                #mask out the points\n",
    "                Y_train_new_masked=torch.masked_select(Y_train_new[:,:,Ynames_n[0]], Y_train_maskb_new[:,:,Ynames_n[0]])\n",
    "                Y_pred_all_masked=torch.masked_select(Y_pred_all[:,:,pred_names_n[0]], Y_train_maskb_new[:,:,Ynames_n[0]])\n",
    "                train_R2=compute_r2(Y_pred_all_masked.contiguous().view(-1),\\\n",
    "                                    Y_train_new_masked.contiguous().view(-1)).item()\n",
    "                ########################validation\n",
    "                Y_val_pred=torch.zeros([Y_val.size(0),Y_val.size(1),len(self.out_names)],device=device)\n",
    "                hidden = model1.init_hidden(X_val.size(1))\n",
    "                #print(stats_val_sq[:,2,0])\n",
    "                Y_val_pt, hidden = model1(X_val,stats_val_sq,hidden)\n",
    "                Y_val_pred[:,:,:] = Y_val_pt[:,:,:]\n",
    "                loss = my_loss_weighted(Y_val_pred[:,:,pred_names_n[0]],Y_val[:,:,Ynames_n[0]],Y_val_mask[:,:,Ynames_n[0]])\n",
    "                val_loss=loss.item()\n",
    "                val_losses.append(val_loss)\n",
    "                Y_val_masked=torch.masked_select(Y_val[:,:,Ynames_n[0]], Y_val_maskb[:,:,Ynames_n[0]])\n",
    "                Y_val_pred_masked=torch.masked_select(Y_val_pred[:,:,pred_names_n[0]], Y_val_maskb[:,:,Ynames_n[0]])\n",
    "                val_R2=compute_r2(Y_val_pred_masked.contiguous().view(-1),Y_val_masked.contiguous().view(-1)).item()\n",
    "                if val_loss < loss_val_best and val_R2 > R2_best:\n",
    "                    loss_val_best=val_loss\n",
    "                    R2_best = val_R2\n",
    "                    f0=open(path_save,'w')\n",
    "                    f0.close()\n",
    "                    #os.remove(path_save)\n",
    "                    torch.save({'epoch': epoch,\n",
    "                            'model_state_dict': model1.state_dict(),\n",
    "                            'R2': train_R2,\n",
    "                            'loss': train_loss,\n",
    "                            'los_val': val_loss,\n",
    "                            'R2_val': val_R2,\n",
    "                            }, path_save)\n",
    "                if epoch%1000 == 999:\n",
    "                    print(\"finished training epoch\", epoch+1)\n",
    "                    mtime=time.time()\n",
    "                    print(\"train_loss: \", train_loss, \"train_R2\", train_R2,\"val_loss:\",val_loss,\"val_R2\", val_R2,\\\n",
    "                          \"loss val best:\",loss_val_best,\"R2 val best:\",R2_best, f\"Spending time: {mtime - starttime}s\")\n",
    "                if train_R2 > 0.99:\n",
    "                    break\n",
    "                #adding early stop\n",
    "            model1.train()\n",
    "        endtime=time.time()\n",
    "        path_fs = path_save+'fs'\n",
    "        torch.save({'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'model_state_dict_fs': model1.state_dict(),\n",
    "                    }, path_fs)\n",
    "        print(\"final train_loss:\",train_loss,\"final train_R2:\",train_R2,\"val_loss:\",val_loss,\"loss validation best:\",loss_val_best)\n",
    "        print(f\"total Training time: {endtime - starttime}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "b4a6b31d-8f3f-45da-aff0-b83d2b75c07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yv/lllpcq5d7g9cxbvg8bj3tg080000gn/T/ipykernel_21980/2058662884.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.tensor(torch.load(self.input_data))[:,:1000,:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6570, 1000, 21])\n",
      "11\n",
      "[ 495 1226 1955 2686 3415 4146 4875 5230 5599 5962 6332]\n",
      "11\n",
      "[ 495 1226 1955 2686 3415 4146 4875 5230 5599 5962 6332]\n",
      "11\n",
      "[ 495 1226 1955 2686 3415 4146 4875 5230 5599 5962 6332]\n"
     ]
    }
   ],
   "source": [
    "#load model input data\n",
    "root_dir = '/Users/yufengyang/Library/CloudStorage/GoogleDrive-yang6956@umn.edu/My Drive/KGML/PyKGML/datasets/N2O mesocosm case/5504533/data/pretrain/'\n",
    "output_path = '/Users/yufengyang/Library/CloudStorage/GoogleDrive-yang6956@umn.edu/My Drive/KGML/PyKGML/datasets/N2O mesocosm case/5504533/test_results/'\n",
    "input_data = 'input16_output5_pretrain_18yr.sav'\n",
    "\n",
    "dataset = KGML_N2O_dataset(input_path=root_dir, output_path=output_path, input_data=input_data)\n",
    "dataset.load_data()\n",
    "dataset.train_test_split(train_ratio=0.8, test_ratio=0.2, val_ratio_to_train=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "56f0640d-b27d-4bb4-a851-aa583c489c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "output_path = '/Users/yufengyang/Library/CloudStorage/GoogleDrive-yang6956@umn.edu/My Drive/KGML/PyKGML/datasets/N2O mesocosm case/5504533/test_results/'\n",
    "pretrained_model_dir = '/Users/yufengyang/Library/CloudStorage/GoogleDrive-yang6956@umn.edu/My Drive/KGML/PyKGML/datasets/N2O mesocosm case/5504533/trained_model_saved/'\n",
    "pretrained_model = pretrained_model_dir + 'n2o_gru_mesotest_v4_exp1.sav'\n",
    "\n",
    "model = KGML_N2O(input_path=root_dir, output_path=output_path, input_data=input_data, pretrained_model=pretrained_model, dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f8c26eee-26a9-4c42-a474-c6b88118bba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statini_sq_N2OGRU(\n",
      "  (gru1): GRU(20, 64, num_layers=2, dropout=0.2)\n",
      "  (gru2): GRU(20, 64, num_layers=2, dropout=0.2)\n",
      "  (densor1): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (densor2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (drop): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "20\n",
      "torch.Size([192, 64])\n",
      "Model's state_dict:\n",
      "gru1.weight_ih_l0 \t torch.Size([192, 20])\n",
      "gru1.weight_hh_l0 \t torch.Size([192, 64])\n",
      "gru1.bias_ih_l0 \t torch.Size([192])\n",
      "gru1.bias_hh_l0 \t torch.Size([192])\n",
      "gru1.weight_ih_l1 \t torch.Size([192, 64])\n",
      "gru1.weight_hh_l1 \t torch.Size([192, 64])\n",
      "gru1.bias_ih_l1 \t torch.Size([192])\n",
      "gru1.bias_hh_l1 \t torch.Size([192])\n",
      "gru2.weight_ih_l0 \t torch.Size([192, 20])\n",
      "gru2.weight_hh_l0 \t torch.Size([192, 64])\n",
      "gru2.bias_ih_l0 \t torch.Size([192])\n",
      "gru2.bias_hh_l0 \t torch.Size([192])\n",
      "gru2.weight_ih_l1 \t torch.Size([192, 64])\n",
      "gru2.weight_hh_l1 \t torch.Size([192, 64])\n",
      "gru2.bias_ih_l1 \t torch.Size([192])\n",
      "gru2.bias_hh_l1 \t torch.Size([192])\n",
      "densor1.weight \t torch.Size([4, 64])\n",
      "densor1.bias \t torch.Size([4])\n",
      "densor2.weight \t torch.Size([1, 64])\n",
      "densor2.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "model_version = 'kgml_n2o_train_2025-02-05'\n",
    "model.load_model(model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "209f958b-4bb4-44ad-8e21-8d150a02f65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training epoch 1\n",
      "train_loss:  0.568976891040802 train_R2 0.9000047445297241 val_loss: 0.5068603754043579 val_R2 0.9202625751495361 loss val best: 0.5068603754043579 R2 val best: 0.9202625751495361 Spending time: 5.184576988220215s\n",
      "finished training epoch 2\n",
      "train_loss:  0.5502346932888031 train_R2 0.9046007990837097 val_loss: 0.4954735040664673 val_R2 0.9194283485412598 loss val best: 0.5068603754043579 R2 val best: 0.9202625751495361 Spending time: 7.349581956863403s\n",
      "finished training epoch 3\n",
      "train_loss:  0.5459117650985718 train_R2 0.9027466177940369 val_loss: 0.48957815766334534 val_R2 0.9225497245788574 loss val best: 0.48957815766334534 R2 val best: 0.9225497245788574 Spending time: 9.515477895736694s\n",
      "finished training epoch 4\n",
      "train_loss:  0.5422211647033691 train_R2 0.9067339897155762 val_loss: 0.46852758526802063 val_R2 0.9241753220558167 loss val best: 0.46852758526802063 R2 val best: 0.9241753220558167 Spending time: 11.66561508178711s\n",
      "finished training epoch 5\n",
      "train_loss:  0.526241946220398 train_R2 0.9092188477516174 val_loss: 0.49028968811035156 val_R2 0.9237003326416016 loss val best: 0.46852758526802063 R2 val best: 0.9241753220558167 Spending time: 13.8642418384552s\n",
      "final train_loss: 0.526241946220398 final train_R2: 0.9092188477516174 val_loss: 0.49028968811035156 loss validation best: 0.46852758526802063\n",
      "total Training time: 13.864290952682495s\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9a29719d-8708-4980-a854-8c199df8880c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total spending time: 0.28369784355163574s\n"
     ]
    }
   ],
   "source": [
    "model.test(model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6220773e-9f58-4848-b424-1987604fd984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yv/lllpcq5d7g9cxbvg8bj3tg080000gn/T/ipykernel_21980/2058662884.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  scalers = torch.load(scaler_path)\n",
      "/var/folders/yv/lllpcq5d7g9cxbvg8bj3tg080000gn/T/ipykernel_21980/2058662884.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data0=torch.load(dataset_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chamber 4 is used to validate\n",
      "(2928, 3, 6, 4) (122, 3, 6, 2) (122, 3, 6, 15) (2928, 3, 6, 5)\n",
      "(15, 5) (3, 5)\n",
      "(122, 1500, 16) (122, 1500, 5) (122, 15, 16) (122, 15, 5) (122, 3, 16) (122, 3, 5)\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/yufengyang/Library/CloudStorage/GoogleDrive-yang6956@umn.edu/My Drive/KGML/PyKGML/datasets/N2O mesocosm case/5504533/data/'\n",
    "dataset_path = data_path + 'finetune/mesotest_data_org_v1.sav'\n",
    "scaler_path = data_path + 'pretrain/input16_output5_scalers.sav'\n",
    "\n",
    "dataset.augment_finetune_data(dataset_path=dataset_path, scaler_path=scaler_path, val_chamber=0, augment_factor=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e26ba0c0-9be8-4caf-a8ce-24f020320b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Using device: mps\n",
      "Statini_sq_N2OGRU(\n",
      "  (gru1): GRU(20, 64, num_layers=2, dropout=0.2)\n",
      "  (gru2): GRU(20, 64, num_layers=2, dropout=0.2)\n",
      "  (densor1): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (densor2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (drop): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "20\n",
      "torch.Size([192, 64])\n",
      "Model's state_dict:\n",
      "gru1.weight_ih_l0 \t torch.Size([192, 20])\n",
      "gru1.weight_hh_l0 \t torch.Size([192, 64])\n",
      "gru1.bias_ih_l0 \t torch.Size([192])\n",
      "gru1.bias_hh_l0 \t torch.Size([192])\n",
      "gru1.weight_ih_l1 \t torch.Size([192, 64])\n",
      "gru1.weight_hh_l1 \t torch.Size([192, 64])\n",
      "gru1.bias_ih_l1 \t torch.Size([192])\n",
      "gru1.bias_hh_l1 \t torch.Size([192])\n",
      "gru2.weight_ih_l0 \t torch.Size([192, 20])\n",
      "gru2.weight_hh_l0 \t torch.Size([192, 64])\n",
      "gru2.bias_ih_l0 \t torch.Size([192])\n",
      "gru2.bias_hh_l0 \t torch.Size([192])\n",
      "gru2.weight_ih_l1 \t torch.Size([192, 64])\n",
      "gru2.weight_hh_l1 \t torch.Size([192, 64])\n",
      "gru2.bias_ih_l1 \t torch.Size([192])\n",
      "gru2.bias_hh_l1 \t torch.Size([192])\n",
      "densor1.weight \t torch.Size([4, 64])\n",
      "densor1.bias \t torch.Size([4])\n",
      "densor2.weight \t torch.Size([1, 64])\n",
      "densor2.bias \t torch.Size([1])\n",
      "Statini_sq_N2OGRU(\n",
      "  (gru1): GRU(20, 64, num_layers=2, dropout=0.2)\n",
      "  (gru2): GRU(20, 64, num_layers=2, dropout=0.2)\n",
      "  (densor1): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (densor2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (drop): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "gru1.weight_ih_l0\n",
      "gru1.weight_hh_l0\n",
      "gru1.bias_ih_l0\n",
      "gru1.bias_hh_l0\n",
      "gru1.weight_ih_l1\n",
      "gru1.weight_hh_l1\n",
      "gru1.bias_ih_l1\n",
      "gru1.bias_hh_l1\n",
      "densor1.weight\n",
      "densor1.bias\n",
      "finished training 1st layer epoch 1000\n",
      "train_loss:  3.6549434661865234 train_R2 0.5028150677680969 val_loss: 3.3523592948913574 val_R2 0.37034672498703003 loss val best: 3.342402458190918 R2 val best: 0.3722696900367737 Spending time: 534.2236449718475s\n",
      "finished training 1st layer epoch 2000\n",
      "train_loss:  3.0279970169067383 train_R2 0.5891678929328918 val_loss: 3.3470747470855713 val_R2 0.37366169691085815 loss val best: 3.2833571434020996 R2 val best: 0.3830410838127136 Spending time: 1068.8751628398895s\n",
      "/Users/yufengyang/Library/CloudStorage/GoogleDrive-yang6956@umn.edu/My Drive/KGML/PyKGML/datasets/N2O mesocosm case/5504533/test_results/n2o_gru_mesotest_val5.sav1l\n",
      "Statini_sq_N2OGRU(\n",
      "  (gru1): GRU(20, 64, num_layers=2, dropout=0.2)\n",
      "  (gru2): GRU(20, 64, num_layers=2, dropout=0.2)\n",
      "  (densor1): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (densor2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (drop): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "gru2.weight_ih_l0\n",
      "gru2.weight_hh_l0\n",
      "gru2.bias_ih_l0\n",
      "gru2.bias_hh_l0\n",
      "gru2.weight_ih_l1\n",
      "gru2.weight_hh_l1\n",
      "gru2.bias_ih_l1\n",
      "gru2.bias_hh_l1\n",
      "densor2.weight\n",
      "densor2.bias\n",
      "torch.Size([122, 1500, 16]) torch.Size([122, 3, 16])\n",
      "final train_loss: 7.194387435913086 final train_R2: 0.4374435544013977 val_loss: 5.5190582275390625 loss validation best: 5.5190582275390625\n",
      "total Training time: 1114.6477739810944s\n"
     ]
    }
   ],
   "source": [
    "print(dataset.augn)\n",
    "basic_path = output_path\n",
    "model_ft = KGML_N2O(input_path=root_dir, output_path=output_path, input_data=input_data, pretrained_model=pretrained_model, dataset=dataset)\n",
    "model_ft.load_model()\n",
    "model_ft.fine_tune(basic_path=basic_path, model_load='n2o_gru_mesotest_v4_exp1.sav', maxepoch_l1 = 2000, maxepoch_l2 = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc4cc2-0b7c-48a2-947f-885bf5177b97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
