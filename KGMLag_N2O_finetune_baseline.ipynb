{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from io import open\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from scipy.stats import gaussian_kde\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2Loss(nn.Module):\n",
    "    #calculate coefficient of determination\n",
    "    def forward(self, y_pred, y):\n",
    "        var_y = torch.var(y, unbiased=False)\n",
    "        return 1.0 - F.mse_loss(y_pred, y, reduction=\"mean\") / var_y\n",
    "\n",
    "import subprocess as sp\n",
    "import os\n",
    "def get_gpu_memory():\n",
    "  _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "\n",
    "  ACCEPTABLE_AVAILABLE_MEMORY = 1024\n",
    "  COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "  memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "  memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "  print(memory_free_values)\n",
    "  return memory_free_values\n",
    "# use N2O model v1--GRU model\n",
    "class N2OGRU(nn.Module):\n",
    "    def __init__(self, ninp, nhid, nlayers, nout, dropout):\n",
    "        super(N2OGRU, self).__init__()\n",
    "        if nlayers > 1:\n",
    "            self.gru = nn.GRU(ninp, nhid,nlayers,dropout=dropout)\n",
    "        else:\n",
    "            self.gru = nn.GRU(ninp, nhid,nlayers)\n",
    "        #self.densor1 = nn.ReLU() #can test other function\n",
    "        self.densor2 = nn.Linear(nhid, nout)\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1 #may change to a small value\n",
    "        self.densor2.bias.data.zero_()\n",
    "        self.densor2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        output, hidden = self.gru(inputs, hidden)\n",
    "        #output = self.densor1(self.drop(output))\n",
    "        #output = torch.exp(self.densor2(self.drop(output))) # add exp\n",
    "        output = self.densor2(self.drop(output)) # add exp\n",
    "        return output, hidden\n",
    "#bsz should be batch size\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
    "    \n",
    "\n",
    "#spin-up: bsz0 is number of year of data_sp you provided for spin up; bsz0=-1 means data_sp=[]\n",
    "#data_sp is the data you provided\n",
    "#return inihidden for simulation period with first year spin-uped\n",
    "def spinup(model,data_sp,cycle,bsz):\n",
    "    inihidden0=model1.init_hidden(bsz)\n",
    "    for c in range(cycle):\n",
    "        output_dummy,inihidden0 = model(data_sp,inihidden0)\n",
    "    return inihidden0\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss\n",
    "#for multi-task learning, sumloss\n",
    "def myloss_mul_sum(output, target,loss_weights):\n",
    "    loss = 0.0\n",
    "    nout=output.size(2)\n",
    "    for i in range(nout):\n",
    "        loss = loss + loss_weights[i]*torch.mean((output[:,:,i] - target[:,:,i])**2)\n",
    "    return loss\n",
    "def scalar_maxmin(X):\n",
    "    return (X - X.min())/(X.max() - X.min()),X.min(),X.max()\n",
    "\n",
    "#generate input combine statini \n",
    "#x should be size of [seq,batch,n_f1], statini be size of [1,batch,n_f2]\n",
    "def load_ini(x,x_ini):\n",
    "    nrep = x.size(0)\n",
    "    x_ini=x_ini[0,:,:].view(1,x_ini.size(1),x_ini.size(2))\n",
    "    return torch.cat((x,x_ini.repeat(nrep,1,1)),2)\n",
    "\n",
    "class Statini_N2OGRU(nn.Module):\n",
    "    #input model variables are for each module\n",
    "    def __init__(self, ninp1, ninp2, nhid, nlayers, nout1, nout2, dropout):\n",
    "        super(Statini_N2OGRU, self).__init__()\n",
    "        if nlayers > 1:\n",
    "            self.gru1 = nn.GRU(ninp1, nhid,nlayers,dropout=dropout)\n",
    "            self.gru2 = nn.GRU(ninp2, nhid,nlayers,dropout=dropout)\n",
    "        else:\n",
    "            self.gru1 = nn.GRU(ninp1, nhid,nlayers)\n",
    "            self.gru2 = nn.GRU(ninp2, nhid,nlayers)\n",
    "        self.densor1 = nn.Linear(nhid, nout1)\n",
    "        self.densor2 = nn.Linear(nhid, nout2)\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1 #may change to a small value\n",
    "        self.densor1.bias.data.zero_()\n",
    "        self.densor1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor2.bias.data.zero_()\n",
    "        self.densor2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, W_inputs, stat_ini, hidden):\n",
    "        inputs = load_ini(W_inputs,stat_ini)\n",
    "        output1, hidden1 = self.gru1(inputs, hidden[0])\n",
    "        output1 = self.densor1(self.drop(output1)) \n",
    "        inputs = torch.cat((W_inputs,output1),2)\n",
    "        output2, hidden2 = self.gru2(inputs, hidden[1])\n",
    "        output2 = self.densor2(self.drop(output2)) \n",
    "        #need to be careful what is the output orders!!!!!!!!!!!!!\n",
    "        output=torch.cat((output2,output1),2)\n",
    "        hidden=(hidden1,hidden2)\n",
    "        return output, hidden\n",
    "#bsz should be batch size\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\\\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "    \n",
    "class Statini_N2OGRU_v2(nn.Module):\n",
    "    #input model variables are for each module\n",
    "    def __init__(self, ninp1, ninp2, nhid, nlayers, nout1, nout2, dropout):\n",
    "        super(Statini_N2OGRU_v2, self).__init__()\n",
    "        if nlayers > 1:\n",
    "            self.gru1 = nn.GRU(ninp1, nhid,nlayers,dropout=dropout)\n",
    "            self.gru2 = nn.GRU(ninp2, nhid,nlayers,dropout=dropout)\n",
    "        else:\n",
    "            self.gru1 = nn.GRU(ninp1, nhid,nlayers)\n",
    "            self.gru2 = nn.GRU(ninp2, nhid,nlayers)\n",
    "        self.densor1 = nn.Linear(nhid, nout1)\n",
    "        self.densor2 = nn.Linear(nhid, nout2)\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1 #may change to a small value\n",
    "        self.densor1.bias.data.zero_()\n",
    "        self.densor1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor2.bias.data.zero_()\n",
    "        self.densor2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, W_inputs, stat_ini,flux_ini, hidden):\n",
    "        inputs = load_ini(W_inputs,stat_ini)\n",
    "        output1, hidden1 = self.gru1(inputs, hidden[0])\n",
    "        output1 = self.densor1(self.drop(output1)) \n",
    "        inputs = torch.cat((W_inputs,output1),2)\n",
    "        inputs = load_ini(inputs,flux_ini)\n",
    "        output2, hidden2 = self.gru2(inputs, hidden[1])\n",
    "        output2 = self.densor2(self.drop(output2)) \n",
    "        #need to be careful what is the output orders!!!!!!!!!!!!!\n",
    "        output=torch.cat((output2,output1),2)\n",
    "        hidden=(hidden1,hidden2)\n",
    "        return output, hidden\n",
    "#bsz should be batch size\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\\\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "    \n",
    "    \n",
    "class Statini_N2OGRU_v3(nn.Module):\n",
    "    #input model variables are for each module\n",
    "    def __init__(self, ninp1, ninp2, nhid1, nhid2, nlayers1, nlayers2, nout1, nout2, dropout):\n",
    "        super(Statini_N2OGRU_v3, self).__init__()\n",
    "        if nlayers1[0] > 1:\n",
    "            self.gru1_1 = nn.GRU(ninp1[0], nhid1[0],nlayers1[0],dropout=dropout)\n",
    "        else:\n",
    "            self.gru1_1 = nn.GRU(ninp1[0], nhid1[0],nlayers1[0])\n",
    "        if nlayers1[1] > 1:\n",
    "            self.gru1_2 = nn.GRU(ninp1[1], nhid1[1],nlayers1[1],dropout=dropout)\n",
    "        else:\n",
    "            self.gru1_2 = nn.GRU(ninp1[1], nhid1[1],nlayers1[1])\n",
    "        if nlayers1[2] > 1:\n",
    "            self.gru1_3 = nn.GRU(ninp1[2], nhid1[2],nlayers1[2],dropout=dropout)\n",
    "        else:\n",
    "            self.gru1_3 = nn.GRU(ninp1[2], nhid1[2],nlayers1[2])\n",
    "        if nlayers1[3] > 1:\n",
    "            self.gru1_4 = nn.GRU(ninp1[3], nhid1[3],nlayers1[3],dropout=dropout)\n",
    "        else:\n",
    "            self.gru1_4 = nn.GRU(ninp1[3], nhid1[3],nlayers1[3])\n",
    "        if nlayers2 > 1:\n",
    "            self.gru2 = nn.GRU(ninp2, nhid2,nlayers2,dropout=dropout)\n",
    "        else:\n",
    "            self.gru2 = nn.GRU(ninp2, nhid2,nlayers2)\n",
    "\n",
    "        self.densor1_1 = nn.Linear(nhid1[0], nout1[0])\n",
    "        self.densor1_2 = nn.Linear(nhid1[1], nout1[1])\n",
    "        self.densor1_3 = nn.Linear(nhid1[2], nout1[2])\n",
    "        self.densor1_4 = nn.Linear(nhid1[3], nout1[3])\n",
    "        self.densor2 = nn.Linear(nhid2, nout2)\n",
    "        self.nhid1 = nhid1\n",
    "        self.nhid2 = nhid2\n",
    "        self.nlayers1 = nlayers1\n",
    "        self.nlayers2 = nlayers2\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1 #may change to a small value\n",
    "        self.densor1_1.bias.data.zero_()\n",
    "        self.densor1_1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor1_2.bias.data.zero_()\n",
    "        self.densor1_2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor1_3.bias.data.zero_()\n",
    "        self.densor1_3.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor1_4.bias.data.zero_()\n",
    "        self.densor1_4.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor2.bias.data.zero_()\n",
    "        self.densor2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, W_inputs, stat_ini,flux_ini, hidden):\n",
    "        #layer 1 for states simulation\n",
    "        inputs = load_ini(W_inputs,stat_ini[0])\n",
    "        output1_1, hidden1_1 = self.gru1_1(inputs, hidden[0][0])\n",
    "        output1_1 = self.densor1_1(self.drop(output1_1))\n",
    "        inputs = load_ini(W_inputs,stat_ini[1])\n",
    "        output1_2, hidden1_2 = self.gru1_2(inputs, hidden[0][1])\n",
    "        output1_2 = self.densor1_2(self.drop(output1_2))\n",
    "        inputs = load_ini(W_inputs,stat_ini[2])\n",
    "        output1_3, hidden1_3 = self.gru1_3(inputs, hidden[0][2])\n",
    "        output1_3 = self.densor1_3(self.drop(output1_3))\n",
    "        inputs = load_ini(W_inputs,stat_ini[3])\n",
    "        output1_4, hidden1_4 = self.gru1_4(inputs, hidden[0][3])\n",
    "        output1_4 = self.densor1_4(self.drop(output1_4))\n",
    "        \n",
    "        inputs = torch.cat((W_inputs,output1_1,output1_2,output1_3,output1_4),2)\n",
    "        inputs = load_ini(inputs,flux_ini)\n",
    "        #layer two for N2O O2 and N2 simulation\n",
    "        output2, hidden2 = self.gru2(inputs, hidden[1])\n",
    "        output2 = self.densor2(self.drop(output2)) \n",
    "        #need to be careful what is the output orders!!!!!!!!!!!!!\n",
    "        output=torch.cat((output2,output1_1,output1_2,output1_3,output1_4),2)\n",
    "        \n",
    "        hidden=((hidden1_1,hidden1_2,hidden1_3,hidden1_4),hidden2)\n",
    "        return output, hidden\n",
    "#bsz should be batch size\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return ((weight.new_zeros(self.nlayers1[0], bsz, self.nhid1[0]),\\\n",
    "                weight.new_zeros(self.nlayers1[1], bsz, self.nhid1[1]),\\\n",
    "                weight.new_zeros(self.nlayers1[2], bsz, self.nhid1[2]),\\\n",
    "                weight.new_zeros(self.nlayers1[3], bsz, self.nhid1[3])),\\\n",
    "                weight.new_zeros(self.nlayers2, bsz, self.nhid2))\n",
    "    \n",
    "class Statini_sq_N2OGRU(nn.Module):\n",
    "    #input model variables are for each module\n",
    "    def __init__(self, ninp1, ninp2, nhid, nlayers, nout1, nout2, dropout):\n",
    "        super(Statini_sq_N2OGRU, self).__init__()\n",
    "        if nlayers > 1:\n",
    "            self.gru1 = nn.GRU(ninp1, nhid,nlayers,dropout=dropout)\n",
    "            self.gru2 = nn.GRU(ninp2, nhid,nlayers,dropout=dropout)\n",
    "        else:\n",
    "            self.gru1 = nn.GRU(ninp1, nhid,nlayers)\n",
    "            self.gru2 = nn.GRU(ninp2, nhid,nlayers)\n",
    "        self.densor1 = nn.Linear(nhid, nout1)\n",
    "        self.densor2 = nn.Linear(nhid, nout2)\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1 #may change to a small value\n",
    "        self.densor1.bias.data.zero_()\n",
    "        self.densor1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.densor2.bias.data.zero_()\n",
    "        self.densor2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, W_inputs, stat_ini_sq, hidden):\n",
    "        inputs = torch.cat((W_inputs,stat_ini_sq),2)\n",
    "        output1, hidden1 = self.gru1(inputs, hidden[0])\n",
    "        output1 = self.densor1(self.drop(output1)) \n",
    "        inputs = torch.cat((W_inputs,output1),2)\n",
    "        output2, hidden2 = self.gru2(inputs, hidden[1])\n",
    "        output2 = self.densor2(self.drop(output2)) \n",
    "        #need to be careful what is the output orders!!!!!!!!!!!!!\n",
    "        output=torch.cat((output2,output1),2)\n",
    "        hidden=(hidden1,hidden2)\n",
    "        return output, hidden\n",
    "    \n",
    "#bsz should be batch size\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\\\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "def get_ini(x,ind,nout):\n",
    "    initials=[]\n",
    "    for i in range(len(ind)):\n",
    "        initials.append(x[:,:,ind[i]].view(x.size(0),x.size(1),nout[i]))\n",
    "    return initials\n",
    "\n",
    "def Z_norm(X):\n",
    "    X_mean=X.mean()\n",
    "    X_std=np.std(np.array(X))\n",
    "    return (X-X_mean)/X_std, X_mean, X_std\n",
    "\n",
    "def Z_norm_reverse(X,Xscaler,units_convert):\n",
    "    return (X*Xscaler[1]+Xscaler[0])*units_convert\n",
    "\n",
    "def Z_norm_with_scaler(X,Xscaler):\n",
    "    return (X-Xscaler[0])/Xscaler[1]\n",
    "\n",
    "#check whether start time is within the fertilized period\n",
    "def dropout_check(start_t,fntime_ind):\n",
    "    dropout_ind=False\n",
    "    for t in fntime_ind:\n",
    "        if start_t > t-10 and start_t < t+60:\n",
    "            dropout_ind=True\n",
    "    return dropout_ind\n",
    "        \n",
    "#sample data considering dropout and leadtime    \n",
    "def sample_data(X,Y,slw,slw05,totsq,fnfeature_ind):\n",
    "    maxit=int((totsq-slw)/slw05+1)\n",
    "    #find the fertilized time\n",
    "    fntime_ind=np.where(X[:,1,fnfeature_ind].view(-1).to(\"cpu\").numpy()>0)[0]\n",
    "    #get sliding window data with dropout method\n",
    "    for it in range(maxit):\n",
    "        if it==0:\n",
    "            X_new = X[slw05*it:slw05*it+slw,:,:]\n",
    "            Y_new = Y[slw05*it:slw05*it+slw,:,:]\n",
    "        else:\n",
    "            if not dropout_check(slw05*it,fntime_ind):\n",
    "                X_new = torch.cat((X_new,X[slw05*it:slw05*it+slw,:,:]),1)\n",
    "                Y_new = torch.cat((Y_new,Y[slw05*it:slw05*it+slw,:,:]),1)\n",
    "    #get focused data only for fertilized period with random leading time\n",
    "    for t in fntime_ind:\n",
    "        for b in range(X.size(1)):\n",
    "            if t != fntime_ind[-1]:\n",
    "                leadtime=np.random.randint(t-60,t-10)\n",
    "        \n",
    "                X_new = torch.cat((X_new,X[leadtime:leadtime+slw,b,:].view(slw,1,X.size(2))),1)\n",
    "                Y_new = torch.cat((Y_new,Y[leadtime:leadtime+slw,b,:].view(slw,1,Y.size(2))),1)\n",
    "    return X_new,Y_new\n",
    "\n",
    "#sample data considering dropout and leadtime    \n",
    "def sample_data_FN(X,Y,totsq,fnfeature_ind):\n",
    "    #find the fertilized time\n",
    "    fntime_ind=np.where(X[:,1,fnfeature_ind].view(-1).to(\"cpu\").numpy()>0)[0]\n",
    "    #get focused data only for fertilized period with random leading time\n",
    "    for t in fntime_ind:\n",
    "        if t == fntime_ind[0]:\n",
    "            X_new = X[t-30:t+90,:,:]\n",
    "            Y_new = Y[t-30:t+90,:,:]\n",
    "        else:\n",
    "            X_new = torch.cat((X_new,X[t-30:t+90,:,:]),1)\n",
    "            Y_new = torch.cat((Y_new,Y[t-30:t+90,:,:]),1)\n",
    "    return X_new,Y_new,fntime_ind\n",
    "\n",
    "def my_loss_weighted(output, target, mask):\n",
    "    loss = torch.mean(((output - target)**2)*mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0.45571126271708373, -1.2270035148495462, -1.9900609770509639, 1.2540460759836642]\n",
      "torch.Size([6570, 1980, 16]) torch.Size([6570, 1980, 5])\n",
      "['FERTZR_N', 'RADN', 'TMAX_AIR', 'TDIF_AIR', 'HMAX_AIR', 'HDIF_AIR', 'WIND', 'PRECN', 'PDOY', 'PLANTT', 'TBKDS', 'TCSAND', 'TCSILT', 'TPH', 'TCEC', 'TSOC']\n",
      "(16, 2) (5, 2)\n"
     ]
    }
   ],
   "source": [
    "#Get the scaler from pretrain data set\n",
    "#prepare input and output\n",
    "start=1\n",
    "end=18\n",
    "Tx=365 #timesteps\n",
    "tyear=end-start+1\n",
    "#out_names=['N2O_FLUX']\n",
    "out_names=['N2O_FLUX','CO2_FLUX','WTR_3','NH4_3','NO3_3']\n",
    "stat_vars=out_names.copy()\n",
    "stat_vars.remove('N2O_FLUX')\n",
    "stat_values=[-1.0,0.2,0.0,20.0]\n",
    "n_out=len(out_names)\n",
    "#'ATM_CO2' constant, AMENDED_C 0, fire n2o,'FIRE_CH4','STG_DEAD, total 25\n",
    "f_names_c=['RESIDUE_C','HUMUS_C','LITTER_C','CO2_FLUX','O2_FLUX','AUTO_RESP','MICRO_C','SURF_RES','CH4_FLUX',\\\n",
    "         'SURF_DOC_FLUX','SUBS_DOC_FLUX','SURF_DIC_FLUX','SUBS_DIC_FLUX','NBP','SOC_1','SOC_3','SOC_5',\\\n",
    "         'H2_FLUX','ECO_HVST_C','ECO_LAI','ECO_GPP','ECO_RA','ECO_NPP','ECO_RH','TTL_DIC']\n",
    "\n",
    "#constant:ACTV_LYR,'SURF_ICE',total 16\n",
    "f_names_w=['ET','RUNOFF','WATER','DISCHG','SNOWPACK','WTR_1','WTR_3','WTR_5','SURF_WTR','ICE_1','ICE_2','ICE_3',\\\n",
    "           'PSI_1','PSI_3','PSI_5','WTR_TBL']\n",
    "\n",
    "\n",
    "#constant:FIRE_N,total 24\n",
    "f_names_n=['RESIDUE_N','HUMUS_N','FERTZR_N','NET_PL_EXCH_N','NH4','NO3','SURF_DON_FLUX','SUBS_DON_FLUX','SURF_DIN_FLUX',\\\n",
    "           'SUBS_DIN_FLUX','N2O_FLUX','NH3_FLUX','N2_FIXN','MICRO_N','NH4_1','NH4_3','NH4_5',\\\n",
    "           'NO3_1','NO3_3','NO3_5','NH4_RES','NO3_RES','ECO_HVST_N','N2_FLUX'] ######### data include the N2O_FLUX!!!!!!!!\n",
    "\n",
    "\n",
    "#constant:,total 19\n",
    "f_names_e=['RADN','TMAX_AIR','TMIN_AIR','HMAX_AIR','HMIN_AIR','WIND','PRECN','TMAX_SOIL_1','TMIN_SOIL_1',\\\n",
    "           'TMAX_SOIL_3','TMIN_SOIL_3','TMAX_SOIL_5','TMIN_SOIL_5','TMAX_LITTER','TMIN_LITTER','ECND_1','ECND_3','ECND_5',\\\n",
    "           'TTL_SALT_DISCHG']\n",
    "\n",
    "#soil property total 15 with variation in new results\n",
    "fp_names=['TSN','FBCU','PDOY','PDS','PDD','DDOY','PLANTT',\\\n",
    "          'LAT','TLB','TBKDS', 'TCSAND', 'TCSILT', 'TPH', 'TCEC', 'TSOC']\n",
    "\n",
    "selected_SP=['PDOY','PLANTT','TBKDS', 'TCSAND', 'TCSILT', 'TPH', 'TCEC', 'TSOC']\n",
    "\n",
    "f_names0=f_names_c+f_names_w+f_names_n+f_names_e+fp_names\n",
    "\n",
    "f_names=['FERTZR_N','RADN','TMAX_AIR','TMIN_AIR','HMAX_AIR','HMIN_AIR','WIND','PRECN']+selected_SP\n",
    "\n",
    "#remove_list=['CO2_FLUX','O2_FLUX','AUTO_RESP','CH4_FLUX','SURF_DOC_FLUX','SUBS_DOC_FLUX',\\\n",
    "#             'SURF_DIC_FLUX','SUBS_DIC_FLUX','H2_FLUX','ECO_GPP','ECO_RA','ECO_NPP','ECO_RH','ET',\\\n",
    "#             'RUNOFF','DISCHG','NET_PL_EXCH_N','SURF_DON_FLUX','SUBS_DON_FLUX','SURF_DIN_FLUX',\\\n",
    "#             'SUBS_DIN_FLUX','N2O_FLUX','NH3_FLUX','N2_FIXN','N2_FLUX','TTL_SALT_DISCHG']\n",
    "#remove_list=['N2O_FLUX']\n",
    "#for c in remove_list:\n",
    "#    f_names.remove(c)\n",
    "\n",
    "\n",
    "n_f0=len(f_names0)\n",
    "n_f=len(f_names)\n",
    "ind=[]\n",
    "for i in range(n_f):\n",
    "    ind.append(f_names0.index(f_names[i]))\n",
    "    \n",
    "#ind=sorted(ind)\n",
    "f_names=[]\n",
    "for i in ind:\n",
    "    f_names.append(f_names0[i])\n",
    "\n",
    "fn_ind=f_names.index('FERTZR_N')\n",
    "print(fn_ind)\n",
    "\n",
    "\n",
    "fln=20 #20 for full 0-300, 15 for 80-240\n",
    "sln=99\n",
    "bsz0=fln*sln\n",
    "X=np.zeros([Tx*tyear,bsz0,n_f0],dtype=np.float32)\n",
    "Y=np.zeros([Tx*tyear,bsz0,n_out],dtype=np.float32)\n",
    "Xscaler=np.zeros([n_f0,2])\n",
    "#load ecosys results\n",
    "basic_path='D:/machinelearning/pgml_progress/mesocosm/'\n",
    "path_load = basic_path+'99points_metrix_scaled1_v9_X_part1.sav'\n",
    "data0=torch.load(path_load)\n",
    "X[:,:,0:45]=data0['InputX']\n",
    "Xscaler[0:45,:]=data0['Xscaler']\n",
    "path_load = basic_path+'99points_metrix_scaled1_v9_X_part2.sav'\n",
    "data0=torch.load(path_load)\n",
    "X[:,:,45:84]=data0['InputX']\n",
    "Xscaler[45:84,:]=data0['Xscaler']\n",
    "#read soil properties:\n",
    "path_load = basic_path+'99points_statv_v3_scaled1.sav'\n",
    "data0=torch.load(path_load)\n",
    "X[:,:,84:n_f0]=data0['Soil_p']\n",
    "Xscaler[84:n_f0,:]=data0['Soil_p_scaler']\n",
    "\n",
    "#use Z-norm to rescale every parameters\n",
    "Yscaler=np.zeros([n_out,2])\n",
    "#Z-norm for Y\n",
    "indout=[]\n",
    "for i in range(n_out):\n",
    "    indout.append(f_names0.index(out_names[i]))\n",
    "Y[:,:,:]=X[:,:,indout]\n",
    "for i in range(n_out):\n",
    "    Y[:,:,i]=(Y[:,:,i]*(Xscaler[indout[i],1]-Xscaler[indout[i],0])+Xscaler[indout[i],0]) # convert back\n",
    "    Y[:,:,i],Yscaler[i,0],Yscaler[i,1]=Z_norm(Y[:,:,i])       \n",
    "#Z-norm for X\n",
    "X=X[:,:,ind]\n",
    "Xscaler=Xscaler[ind,:]\n",
    "for i in range(len(ind)):\n",
    "    X[:,:,i]=(X[:,:,i]*(Xscaler[i,1]-Xscaler[i,0])+Xscaler[i,0]) # convert back\n",
    "    X[:,:,i],Xscaler[i,0],Xscaler[i,1]=Z_norm(X[:,:,i])  \n",
    "\n",
    "Y=torch.from_numpy(Y)\n",
    "X=torch.from_numpy(X)\n",
    "\n",
    "#need to change to new Z_norm\n",
    "#W_names=['TMAX_AIR','TMIN_AIR','HMAX_AIR','HMIN_AIR','TMAX_SOIL_1','TMIN_SOIL_1',\\\n",
    "#         'TMAX_SOIL_3','TMIN_SOIL_3','TMAX_SOIL_5','TMIN_SOIL_5','TMAX_LITTER','TMIN_LITTER']\n",
    "#Diff_names=['TDIF_AIR','HDIF_AIR','TDIF_SOIL_1','TDIF_SOIL_3','TDIF_SOIL_5','TDIF_LITTER']\n",
    "#need to change to new Z_norm\n",
    "W_names=['TMAX_AIR','TMIN_AIR','HMAX_AIR','HMIN_AIR']\n",
    "Diff_names=['TDIF_AIR','HDIF_AIR']\n",
    "W_ind=[]\n",
    "for i in range(len(W_names)):\n",
    "    W_ind.append(f_names.index(W_names[i]))\n",
    "#replace max-min\n",
    "for i in range(len(Diff_names)):\n",
    "    Vmax=X[:,:,W_ind[i*2]]*Xscaler[W_ind[i*2],1]+Xscaler[W_ind[i*2],0]\n",
    "    Vmin=X[:,:,W_ind[i*2+1]]*Xscaler[W_ind[i*2+1],1]+Xscaler[W_ind[i*2+1],0] \n",
    "    X[:,:,W_ind[i*2+1]]=Vmax-Vmin\n",
    "    X[:,:,W_ind[i*2+1]],Xscaler[W_ind[i*2+1],0],Xscaler[W_ind[i*2+1],1]=Z_norm(X[:,:,W_ind[i*2+1]])\n",
    "    f_names[W_ind[i*2+1]]=Diff_names[i]\n",
    "#scale the initials\n",
    "for i in range(len(stat_values)):\n",
    "    stat_values[i]=Z_norm_with_scaler(stat_values[i],Yscaler[i+1,:])\n",
    "print(stat_values)\n",
    "\n",
    "print(X.size(),Y.size())\n",
    "print(f_names)\n",
    "print(Xscaler.shape,Yscaler.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "(2928, 3, 6, 4) (122, 3, 6, 2) (122, 3, 6, 15) (2928, 3, 6, 5)\n",
      "(15, 5) (3, 5)\n",
      "(122, 15000, 16) (122, 15000, 5) (122, 15, 16) (122, 15, 5) (122, 3, 16) (122, 3, 5)\n",
      "(16, 2) (5, 2) 16\n",
      "cuda\n",
      "torch.Size([122, 15000, 16]) tensor(-514441.6250, device='cuda:0')\n",
      "[5158]\n",
      "Statini_sq_N2OGRU(\n",
      "  (gru1): GRU(20, 64, num_layers=2, dropout=0.2)\n",
      "  (gru2): GRU(20, 64, num_layers=2, dropout=0.2)\n",
      "  (densor1): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (densor2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (drop): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "gru1.weight_ih_l0\n",
      "gru1.weight_hh_l0\n",
      "gru1.bias_ih_l0\n",
      "gru1.bias_hh_l0\n",
      "gru1.weight_ih_l1\n",
      "gru1.weight_hh_l1\n",
      "gru1.bias_ih_l1\n",
      "gru1.bias_hh_l1\n",
      "densor1.weight\n",
      "densor1.bias\n",
      "torch.Size([122, 15, 16]) torch.Size([122, 3, 16])\n",
      "finished training 1st layer epoch 1000\n",
      "train_loss:  3.0660083293914795 train_R2 0.5410485863685608 val_loss: 4.032071113586426 val_R2 0.5333629846572876 loss val best: 4.030301570892334 R2 val best: 0.5335549116134644 Spending time: 19.749913930892944s\n",
      "finished training 1st layer epoch 2000\n",
      "train_loss:  2.0532994270324707 train_R2 0.6944686770439148 val_loss: 4.141461372375488 val_R2 0.5229023098945618 loss val best: 3.8714349269866943 R2 val best: 0.5529742240905762 Spending time: 38.56581091880798s\n",
      "finished training 1st layer epoch 3000\n",
      "train_loss:  1.682983636856079 train_R2 0.7496094107627869 val_loss: 4.863116264343262 val_R2 0.4399959444999695 loss val best: 3.8714349269866943 R2 val best: 0.5529742240905762 Spending time: 57.16421294212341s\n",
      "finished training 1st layer epoch 4000\n",
      "train_loss:  1.409347653388977 train_R2 0.7902809977531433 val_loss: 6.954124927520752 val_R2 0.19964760541915894 loss val best: 3.8714349269866943 R2 val best: 0.5529742240905762 Spending time: 75.73701405525208s\n",
      "finished training 1st layer epoch 5000\n",
      "train_loss:  1.229241967201233 train_R2 0.8170673251152039 val_loss: 7.1424150466918945 val_R2 0.17802411317825317 loss val best: 3.8714349269866943 R2 val best: 0.5529742240905762 Spending time: 94.27336812019348s\n",
      "finished training 1st layer epoch 6000\n",
      "train_loss:  1.0646979808807373 train_R2 0.8414533138275146 val_loss: 6.264114856719971 val_R2 0.2790289521217346 loss val best: 3.8714349269866943 R2 val best: 0.5529742240905762 Spending time: 113.08256983757019s\n",
      "finished training 1st layer epoch 7000\n",
      "train_loss:  0.8468134999275208 train_R2 0.8738452196121216 val_loss: 5.687702655792236 val_R2 0.3452971577644348 loss val best: 3.8714349269866943 R2 val best: 0.5529742240905762 Spending time: 132.082825422287s\n",
      "finished training 1st layer epoch 8000\n",
      "train_loss:  0.6761665344238281 train_R2 0.8990936875343323 val_loss: 4.832230567932129 val_R2 0.44370490312576294 loss val best: 3.8714349269866943 R2 val best: 0.5529742240905762 Spending time: 150.73186421394348s\n",
      "finished training 1st layer epoch 9000\n",
      "train_loss:  0.6535132527351379 train_R2 0.9025518298149109 val_loss: 4.93005895614624 val_R2 0.4324879050254822 loss val best: 3.8714349269866943 R2 val best: 0.5529742240905762 Spending time: 169.5428490638733s\n",
      "finished training 1st layer epoch 10000\n",
      "train_loss:  0.5089576840400696 train_R2 0.9239085912704468 val_loss: 4.982156753540039 val_R2 0.4265051484107971 loss val best: 3.8714349269866943 R2 val best: 0.5529742240905762 Spending time: 188.1983449459076s\n",
      "finished training 1st layer epoch 11000\n",
      "train_loss:  0.45865193009376526 train_R2 0.9314584732055664 val_loss: 4.911307334899902 val_R2 0.43465960025787354 loss val best: 3.8714349269866943 R2 val best: 0.5529742240905762 Spending time: 206.8551299571991s\n",
      "finished training 1st layer epoch 12000\n",
      "train_loss:  0.48594504594802856 train_R2 0.9273648262023926 val_loss: 4.876158714294434 val_R2 0.4386906623840332 loss val best: 3.8714349269866943 R2 val best: 0.5529742240905762 Spending time: 225.41311144828796s\n",
      "finished training 1st layer epoch 13000\n",
      "train_loss:  0.48197317123413086 train_R2 0.9279990196228027 val_loss: 4.764571189880371 val_R2 0.45151811838150024 loss val best: 3.8714349269866943 R2 val best: 0.5529742240905762 Spending time: 244.174907207489s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-cc60379f7a2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#####################prepare the retrain data\n",
    "# for k-fold cross validation, pretest for speed\n",
    "\n",
    "path_load = basic_path + 'mesotest_data_org_v1.sav'\n",
    "data0=torch.load(path_load)\n",
    "X1=data0['InputX1']\n",
    "X2=data0['InputX2']\n",
    "X3=data0['Soil_p']\n",
    "Y=data0['OutputY']\n",
    "days=122\n",
    "nyear=3\n",
    "totnchamber=6\n",
    "c_index=[0,1,2,3,4,5]\n",
    "#######################################k-fold choose \n",
    "\n",
    "for val_cn in c_index:\n",
    "    c_val = [val_cn]\n",
    "    nc_val = len(c_val)\n",
    "    c_train= [x for i,x in enumerate(c_index) if i not in c_val] \n",
    "    nc_train = len(c_train)\n",
    "    print(c_train)\n",
    "    augn=1000\n",
    "    print(X1.shape,X2.shape,X3.shape,Y.shape)\n",
    "    pred_names=['N2O_FLUX','CO2_FLUX','NO3_3','NH4_3','WTR_3']\n",
    "    #load data n\n",
    "    Ynames_n = [0,1,2,3,4]\n",
    "    #find the pred_names number in out_names, \n",
    "    #the no. of model output Y_train_pred[pred_names_n[i]] will be the related to Y_train[Ynames_n[i]]\n",
    "    pred_names_n = []\n",
    "    for i in range(len(pred_names)):\n",
    "        pred_names_n.append(out_names.index(pred_names[i]))\n",
    "        \n",
    "        \n",
    "    X1names = ['tair','swdown','precip','spRH'] \n",
    "    X2names = ['Obs_prec','Fertilizer']\n",
    "    X3names=['TSN','FBCU','PDOY','PDS','PDD','DDOY','PLANTT',\\\n",
    "              'LAT','TLB','TBKDS', 'TCSAND', 'TCSILT', 'TPH', 'TCEC', 'TSOC']\n",
    "    Ynames= ['N2O_FLUX','CO2_FLUX','NO3','NH4','WFPS']\n",
    "    Y_units_convert=[-24.0,-24.0,1.0,1.0,(1-1.5/2.65)/100.0]\n",
    "\n",
    "    X_train = np.zeros([days,augn*nyear*nc_train,len(f_names)],dtype=np.float32)\n",
    "    Y_train = np.zeros([days,augn*nyear*nc_train,len(Ynames)],dtype=np.float32)\n",
    "    Y_train_mask=np.zeros(Y_train.shape,dtype=np.float32)\n",
    "    #for training without augmentation\n",
    "    X_train_d = np.zeros([days,nyear*nc_train,len(f_names)],dtype=np.float32)\n",
    "    Y_train_d = np.zeros([days,nyear*nc_train,len(Ynames)],dtype=np.float32)\n",
    "    Y_train_d_mask=np.zeros(Y_train_d.shape,dtype=np.float32) \n",
    "    \n",
    "    X_val=np.zeros([days,nyear*nc_val,len(f_names)],dtype=np.float32)\n",
    "    Y_val=np.zeros([days,nyear*nc_val,len(Ynames)],dtype=np.float32)\n",
    "    Y_val_mask=np.zeros(Y_val.shape,dtype=np.float32) \n",
    "\n",
    "    \n",
    "    #Y_gt ground truth first day index, for initials creating\n",
    "    Y_train_gt_1stind = np.zeros([nyear*nc_train,len(Ynames)], dtype=int)\n",
    "    Y_val_gt_1stind = np.zeros([nyear*nc_val,len(Ynames)], dtype=int)\n",
    "    print(Y_train_gt_1stind.shape,Y_val_gt_1stind.shape)\n",
    "    #Method: Multidimensional Shifting using NumPy\n",
    "    #method from https://ethankoch.medium.com/incredibly-fast-random-sampling-in-python-baf154bd836a\n",
    "    #product index_array (num_samples,sample_size) within elements\n",
    "    # constants\n",
    "    # returning index\n",
    "    num_samples = augn\n",
    "    sample_size = 16 #sample 16 hours within one day\n",
    "    num_elements = 24\n",
    "    #elements = np.arange(num_elements)\n",
    "    # probabilities should sum to 1\n",
    "    probabilities = np.random.random(num_elements)\n",
    "    probabilities /= np.sum(probabilities)\n",
    "    def multidimensional_shifting(num_samples, sample_size, probabilities):\n",
    "        # replicate probabilities as many times as `num_samples`\n",
    "        replicated_probabilities = np.tile(probabilities, (num_samples, 1))\n",
    "        # get random shifting numbers & scale them correctly\n",
    "        random_shifts = np.random.random(replicated_probabilities.shape)\n",
    "        random_shifts /= random_shifts.sum(axis=1)[:, np.newaxis]\n",
    "        # shift by numbers & find largest (by finding the smallest of the negative)\n",
    "        shifted_probabilities = random_shifts - replicated_probabilities\n",
    "        return np.argpartition(shifted_probabilities, sample_size, axis=1)[:, :sample_size]\n",
    "\n",
    "    #sample data from mesocosm site chambers\n",
    "    for d in range(days):\n",
    "        #for training data with data augmentation\n",
    "        for y in range(nyear):\n",
    "            for c in range(nc_train):\n",
    "                #get random sampled indexes\n",
    "                sample_indexes = multidimensional_shifting(num_samples, sample_size, probabilities)\n",
    "                #input data\n",
    "                #temperature\n",
    "                elements = np.tile(X1[d*24:(d+1)*24,y,c_train[c],0], (num_samples, 1)) # copy the hourly data num_samples times\n",
    "                output_samples = np.take_along_axis(elements, sample_indexes, axis=1) # sample the data based on random indexes\n",
    "                output_samples_tmax = output_samples.max(1)\n",
    "                output_samples_tdif = output_samples_tmax-output_samples.min(1)\n",
    "                X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),2]=output_samples_tmax\n",
    "                X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),3]=output_samples_tdif\n",
    "                X_train_d[d,y*nc_train+c,2] = np.max(X1[d*24:(d+1)*24,y,c_train[c],0])\n",
    "                X_train_d[d,y*nc_train+c,3] = np.max(X1[d*24:(d+1)*24,y,c_train[c],0])-\\\n",
    "                                                        np.min(X1[d*24:(d+1)*24,y,c_train[c],0])\n",
    "                #radiation need to convert from W/m-2 to MJ m-2 d-1, *3600*24*10-6\n",
    "                elements = np.tile(X1[d*24:(d+1)*24,y,c_train[c],1], (num_samples, 1)) # copy the hourly data num_samples times\n",
    "                output_samples = np.take_along_axis(elements, sample_indexes, axis=1) # sample the data based on random indexes\n",
    "                output_samples_rad = output_samples.mean(1)*(3600.0*24.0*(10**(-6)))\n",
    "                X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),1]=output_samples_rad\n",
    "                X_train_d[d,y*nc_train+c,1] = np.mean(X1[d*24:(d+1)*24,y,c_train[c],1])*(3600.0*24.0*(10**(-6)))\n",
    "                #humidity\n",
    "                elements = np.tile(X1[d*24:(d+1)*24,y,c_train[c],3], (num_samples, 1)) # copy the hourly data num_samples times\n",
    "                output_samples = np.take_along_axis(elements, sample_indexes, axis=1) # sample the data based on random indexes\n",
    "                output_samples_hmax = output_samples.max(1)\n",
    "                output_samples_hdif = output_samples_hmax - output_samples.min(1)\n",
    "                X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),4]=output_samples_hmax\n",
    "                X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),5]=output_samples_hdif\n",
    "                X_train_d[d,y*nc_train+c,4] = np.max(X1[d*24:(d+1)*24,y,c_train[c],3])\n",
    "                X_train_d[d,y*nc_train+c,5] = np.max(X1[d*24:(d+1)*24,y,c_train[c],3]) - \\\n",
    "                                                        np.min(X1[d*24:(d+1)*24,y,c_train[c],3])\n",
    "                #sample Y data\n",
    "                for ffy in range(len(Ynames)): \n",
    "                    element=Y[d*24:(d+1)*24,y,c_train[c],ffy]\n",
    "                    nan_nums=np.count_nonzero(np.isnan(element))\n",
    "                    if  nan_nums < 16:\n",
    "                        # copy the hourly data num_samples times\n",
    "                        elements = np.tile(element, (num_samples, 1)) \n",
    "                        # sample the data based on random indexes\n",
    "                        output_samples = np.take_along_axis(elements, sample_indexes, axis=1) \n",
    "                        #convert to right units (n2O g N m-2 h-1 to d-1)\n",
    "                        output_samples_n2o = np.nanmean(output_samples,axis=1)\n",
    "                        # need to be direction to soil\n",
    "                        Y_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),ffy]= output_samples_n2o*Y_units_convert[ffy]\n",
    "                        Y_train_mask[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),ffy] = (24.0-float(nan_nums))/24.0\n",
    "                        \n",
    "                        Y_train_d[d,y*nc_train+c,ffy]= np.nanmean(Y[d*24:(d+1)*24,y,c_train[c],ffy])*\\\n",
    "                                                        Y_units_convert[ffy] #convert \n",
    "                        Y_train_d_mask[d,y*nc_train+c,ffy] = (24.0-float(nan_nums))/24.0\n",
    "                        #get the first day of ground truth\n",
    "                        if Y_train_gt_1stind[y*nc_train+c,ffy] == 0:\n",
    "                            Y_train_gt_1stind[y*nc_train+c,ffy] = d\n",
    "\n",
    "                    else:\n",
    "                        # if missing value >=16, we use -999 represent nan\n",
    "                        Y_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),ffy]=-999.0 \n",
    "                        Y_train_mask[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),ffy] = 0.0\n",
    "                        \n",
    "                        Y_train_d[d,y*nc_train+c,ffy]=-999.0 \n",
    "                        Y_train_d_mask[d,y*nc_train+c,ffy] = 0.0\n",
    "                #deal with other training variables\n",
    "                #fertilizer\n",
    "                X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),0] = X2[d,y,c_train[c],1]\n",
    "                X_train_d[d,y*nc_train+c,0] = X2[d,y,c_train[c],1]\n",
    "                #wind\n",
    "                X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),6] = 0.05\n",
    "                X_train_d[d,y*nc_train+c,6] = 0.05\n",
    "                #precipitation\n",
    "                X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),7] = X2[d,y,c_train[c],0]\n",
    "                X_train_d[d,y*nc_train+c,7] = X2[d,y,c_train[c],0]\n",
    "                for i in range(len(selected_SP)):\n",
    "                    X_train[d,augn*(y*nc_train+c):augn*(y*nc_train+c+1),8+i] = X3[d,y,c_train[c],X3names.index(selected_SP[i])]\n",
    "                    X_train_d[d,y*nc_train+c,8+i] = X3[d,y,c_train[c],X3names.index(selected_SP[i])]\n",
    "\n",
    "\n",
    "    #load the validation\n",
    "    for d in range(days):\n",
    "        for y in range(nyear):\n",
    "            for c in range(nc_val):\n",
    "                #temperature\n",
    "                X_val[d,y*nc_val+c,2] = np.max(X1[d*24:(d+1)*24,y,c_val[c],0])\n",
    "                X_val[d,y*nc_val+c,3] = np.max(X1[d*24:(d+1)*24,y,c_val[c],0])-\\\n",
    "                                                        np.min(X1[d*24:(d+1)*24,y,c_val[c],0])\n",
    "                #radiation\n",
    "                X_val[d,y*nc_val+c,1] = np.mean(X1[d*24:(d+1)*24,y,c_val[c],1])*(3600.0*24.0*(10**(-6)))\n",
    "                #humidity\n",
    "                X_val[d,y*nc_val+c,4] = np.max(X1[d*24:(d+1)*24,y,c_val[c],3])\n",
    "                X_val[d,y*nc_val+c,5] = np.max(X1[d*24:(d+1)*24,y,c_val[c],3]) - \\\n",
    "                                                        np.min(X1[d*24:(d+1)*24,y,c_val[c],3])\n",
    "                #Y data\n",
    "                for ffy in range(len(Ynames)): \n",
    "                    element = Y[d*24:(d+1)*24,y,c_val[c],ffy]\n",
    "                    nan_nums=np.count_nonzero(np.isnan(element))\n",
    "                    if  nan_nums < 16:\n",
    "                        Y_val[d,y*nc_val+c,ffy] = np.nanmean(element)*Y_units_convert[ffy] #convert \n",
    "                        Y_val_mask[d,y*nc_val+c,ffy] = (24.0-float(nan_nums))/24.0\n",
    "                        #get the first day of ground truth\n",
    "                        if Y_val_gt_1stind[y*nc_val+c,ffy] == 0:\n",
    "                            Y_val_gt_1stind[y*nc_val+c,ffy] = d\n",
    "                    else:\n",
    "                        Y_val[d,y*nc_val+c,ffy] = -999.0 # if missing value >=16, we use -999 represent nan\n",
    "                        Y_val_mask[d,y*nc_val+c,ffy] = 0.0\n",
    "                #deal with other training variables\n",
    "                #fertilizer\n",
    "                X_val[d,y*nc_val+c,0] = X2[d,y,c_val[c],1]\n",
    "                #wind\n",
    "                X_val[d,y*nc_val+c,6] = 0.05\n",
    "                #precipitation\n",
    "                X_val[d,y*nc_val+c,7] = X2[d,y,c_val[c],0]\n",
    "                for i in range(len(selected_SP)):\n",
    "                    X_val[d,y*nc_val+c,8+i] = X3[d,y,c_val[c],X3names.index(selected_SP[i])]\n",
    "\n",
    "    print(X_train.shape,Y_train.shape,X_train_d.shape,Y_train_d.shape,X_val.shape,Y_val.shape)\n",
    "    print(Xscaler.shape, Yscaler.shape,X_train.shape[2])\n",
    "    #Z-norm the matrix\n",
    "    for i in range(X_train.shape[2]):\n",
    "        X_train[:,:,i]=Z_norm_with_scaler(X_train[:,:,i],Xscaler[i,:])\n",
    "        X_train_d[:,:,i]=Z_norm_with_scaler(X_train_d[:,:,i],Xscaler[i,:])\n",
    "        X_val[:,:,i]=Z_norm_with_scaler(X_val[:,:,i],Xscaler[i,:])\n",
    "    for i in range(len(Ynames_n)):\n",
    "        Y_train[:,:,Ynames_n[i]]=Z_norm_with_scaler(Y_train[:,:,Ynames_n[i]],Yscaler[pred_names_n[i],:])\n",
    "        Y_train_d[:,:,Ynames_n[i]]=Z_norm_with_scaler(Y_train_d[:,:,Ynames_n[i]],Yscaler[pred_names_n[i],:])\n",
    "        Y_val[:,:,Ynames_n[i]]=Z_norm_with_scaler(Y_val[:,:,Ynames_n[i]],Yscaler[pred_names_n[i],:])\n",
    "\n",
    "    \n",
    "    #transfer to cuda\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    print(device)  \n",
    "\n",
    "    X_train = torch.from_numpy(X_train).to(device)\n",
    "    Y_train = torch.from_numpy(Y_train).to(device)\n",
    "    X_train_d = torch.from_numpy(X_train_d).to(device)\n",
    "    Y_train_d = torch.from_numpy(Y_train_d).to(device)\n",
    "    X_val = torch.from_numpy(X_val).to(device)\n",
    "    Y_val = torch.from_numpy(Y_val).to(device)\n",
    "    #transfer mask to the cuda\n",
    "    Y_train_mask = torch.from_numpy(Y_train_mask).to(device)\n",
    "    Y_val_mask = torch.from_numpy(Y_val_mask).to(device)\n",
    "    Y_train_maskb = Y_train_mask.ge(0.25)\n",
    "    Y_val_maskb = Y_val_mask.ge(0.25)\n",
    "    \n",
    "    Y_train_d_mask = torch.from_numpy(Y_train_d_mask).to(device)\n",
    "    Y_train_d_maskb = Y_train_d_mask.ge(0.25)\n",
    "\n",
    "    print(X_train.size(),Y_train.min())\n",
    "    get_gpu_memory()\n",
    "    \n",
    "    #print(Y_train_gt_1stind,'01!!!!!!!!!!!!!!!!')\n",
    "    #print(Y_val_gt_1stind,'02!!!!!!!!!!!!!!!!')\n",
    "    \n",
    "    \n",
    "    #retrain the model\n",
    "    #########################prepare the fake initials\n",
    "    #Generate initials considering the ground truth:\n",
    "    #Y_gt is the ground truth, using for validate or train\n",
    "    def generate_ini_stats(stat_values,Y_gt,Y_gt_1stind,aug_n,pred_names_n,Ynames_n):\n",
    "        statini_sq = torch.zeros([Y_gt.size(0),Y_gt.size(1),len(stat_values)],device=device)\n",
    "        for i in range(len(stat_values)):\n",
    "            statini_sq[:,:,i]=stat_values[i]\n",
    "        for i in range(len(pred_names_n)):\n",
    "            for cc in range(len(Y_gt_1stind[:,Ynames_n[i]])):\n",
    "                if Y_gt_1stind[cc,Ynames_n[i]] != 0:\n",
    "                    statini_sq[Y_gt_1stind[cc,Ynames_n[i]]:,cc*aug_n:(cc+1)*aug_n,pred_names_n[i]-1]= \\\n",
    "                                                        Y_gt[Y_gt_1stind[cc,Ynames_n[i]],cc*aug_n:(cc+1)*aug_n,Ynames_n[i]].\\\n",
    "                                                        view(1,aug_n).repeat(Y_gt.size(0)-Y_gt_1stind[cc,Ynames_n[i]],1)\n",
    "        return statini_sq\n",
    "    \n",
    "    #generate initials\n",
    "    stats_train_sq = generate_ini_stats(stat_values,Y_train,\\\n",
    "                                        Y_train_gt_1stind,augn,pred_names_n[1:],Ynames_n[1:])\n",
    "    stats_val_sq = generate_ini_stats(stat_values,Y_val,\\\n",
    "                                                Y_val_gt_1stind,1,pred_names_n[1:],Ynames_n[1:])\n",
    "    \n",
    "    #generate initials for training without \n",
    "    stats_train_d_sq = generate_ini_stats(stat_values,Y_train_d,\\\n",
    "                                        Y_train_gt_1stind,1,pred_names_n[1:],Ynames_n[1:])\n",
    "\n",
    "    \n",
    "\n",
    "    def generate_fake_stats(stat_values,batch):\n",
    "        fake_stats=torch.zeros([1,batch,len(stat_values)],device=device)\n",
    "        for i in range(len(stat_values)):\n",
    "            fake_stats[:,:,i]=stat_values[i]\n",
    "        return fake_stats\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################train the model start\n",
    "    model_version=\"n2o_gru_mesotest_v4_exp7\"+\"_val\"+str(c_val[0]+1)+\".sav\"\n",
    "\n",
    "    ####################freeze the second layer, only train the first layer\n",
    "    #only train first layer and states variables and without augmentation\n",
    "    ###load model\n",
    "    n_a=64 #hidden state number\n",
    "    n_l=2 #layer of lstm\n",
    "    dropout=0.2\n",
    "    model_load='n2o_gru_mesotest_v4_exp1.sav'\n",
    "    path_load = basic_path+model_load\n",
    "    checkpoint=torch.load(path_load)\n",
    "    model1=Statini_sq_N2OGRU(n_f+len(stat_values),n_f+len(stat_values),n_a,n_l,len(stat_values),1,dropout)\n",
    "    model1.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model1.to(device) #too large for GPU, kif not enough, change to cpu\n",
    "    print(model1)\n",
    "    loss_val_best = 500000\n",
    "    R2_best=0.2\n",
    "    compute_r2=R2Loss()\n",
    "    path_save = basic_path+model_version+'1l'\n",
    "    \n",
    "    ##freeze 2nd layer\n",
    "    for name, param in model1.named_parameters():\n",
    "        if param.requires_grad and ('gru2' in name or 'densor2' in name):\n",
    "            param.requires_grad = False\n",
    "        if param.requires_grad:\n",
    "            print(name)\n",
    "            \n",
    "    \n",
    "    starttime=time.time()\n",
    "    lr=0.1 #sgd\n",
    "    lr_adam=0.0001*0.5\n",
    "    optimizer = optim.Adam(model1.parameters(), lr=lr_adam) #add weight decay normally 1-9e-4\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.5)\n",
    "    slw=122\n",
    "    print(X_train_d.size(),X_val.size())\n",
    "    batch_total=X_train_d.size(1)\n",
    "    batch_size=batch_total  # this is the batch size for training\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    maxepoch=20000\n",
    "    model1.train()\n",
    "    \n",
    "    for epoch in range(maxepoch):\n",
    "        train_loss=0.0\n",
    "        val_loss=0.0\n",
    "        Y_pred_all=torch.zeros([Y_train_d.size(0),Y_train_d.size(1),len(out_names)],device=device)\n",
    "        #shuffled the training data\n",
    "        shuffled_b=torch.randperm(X_train_d.size()[1]) \n",
    "        X_train_new=X_train_d[:,shuffled_b,:] \n",
    "        Y_train_new=Y_train_d[:,shuffled_b,:]\n",
    "        Y_train_mask_new = Y_train_d_mask[:,shuffled_b,:]\n",
    "        Y_train_maskb_new = Y_train_d_maskb[:,shuffled_b,:]\n",
    "        stats_train_sq_new = stats_train_d_sq[:,shuffled_b,:]  \n",
    "        model1.zero_grad()\n",
    "        for bb in range(int(batch_total/batch_size)):\n",
    "            hidden = model1.init_hidden(batch_size)\n",
    "            Y_pred,hidden = model1(X_train_new[:,bb*batch_size:(bb+1)*batch_size,:],\\\n",
    "                                   stats_train_sq_new[:,bb*batch_size:(bb+1)*batch_size,:],hidden)\n",
    "            #need to adjust the loss for missing data based on mask\n",
    "            loss = my_loss_weighted(Y_pred[:,:,pred_names_n[1:]], \\\n",
    "                                    Y_train_new[:,bb*batch_size:(bb+1)*batch_size,Ynames_n[1:]],\\\n",
    "                                    Y_train_mask_new[:,bb*batch_size:(bb+1)*batch_size,Ynames_n[1:]])\n",
    "            hidden[0].detach_() \n",
    "            hidden[1].detach_()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                train_loss=train_loss+loss.item()\n",
    "                Y_pred_all[:,bb*batch_size:(bb+1)*batch_size,:]=Y_pred[:,:,:]\n",
    "       \n",
    "    \n",
    "        scheduler.step()\n",
    "        #validation\n",
    "        model1.eval()\n",
    "        with torch.no_grad():\n",
    "            train_loss=train_loss/(batch_total/batch_size)\n",
    "            train_losses.append(train_loss)\n",
    "            #mask out the points\n",
    "            Y_train_new_masked=torch.masked_select(Y_train_new[:,:,Ynames_n[1:]], Y_train_maskb_new[:,:,Ynames_n[1:]])\n",
    "            Y_pred_all_masked=torch.masked_select(Y_pred_all[:,:,pred_names_n[1:]], Y_train_maskb_new[:,:,Ynames_n[1:]])\n",
    "            train_R2=compute_r2(Y_pred_all_masked.contiguous().view(-1),\\\n",
    "                                Y_train_new_masked.contiguous().view(-1)).item()\n",
    "            ########################validation\n",
    "            Y_val_pred=torch.zeros([Y_val.size(0),Y_val.size(1),len(out_names)],device=device)\n",
    "            hidden = model1.init_hidden(X_val.size(1))\n",
    "            #print(stats_val_sq[:,2,0])\n",
    "            Y_val_pt, hidden = model1(X_val,stats_val_sq,hidden)\n",
    "            Y_val_pred[:,:,:] = Y_val_pt[:,:,:]\n",
    "            loss = my_loss_weighted(Y_val_pred[:,:,pred_names_n[1:]],Y_val[:,:,Ynames_n[1:]],Y_val_mask[:,:,Ynames_n[1:]])\n",
    "            val_loss=loss.item()\n",
    "            val_losses.append(val_loss)\n",
    "            Y_val_masked=torch.masked_select(Y_val[:,:,Ynames_n[1:]], Y_val_maskb[:,:,Ynames_n[1:]])\n",
    "            Y_val_pred_masked=torch.masked_select(Y_val_pred[:,:,pred_names_n[1:]], Y_val_maskb[:,:,Ynames_n[1:]])\n",
    "            val_R2=compute_r2(Y_val_pred_masked.contiguous().view(-1),Y_val_masked.contiguous().view(-1)).item()\n",
    "            if val_loss < loss_val_best and val_R2 > R2_best:\n",
    "                loss_val_best=val_loss\n",
    "                R2_best = val_R2\n",
    "                f0=open(path_save,'w')\n",
    "                f0.close()\n",
    "                #os.remove(path_save)\n",
    "                torch.save({'epoch': epoch,\n",
    "                        'model_state_dict': model1.state_dict(),\n",
    "                        'R2': train_R2,\n",
    "                        'loss': train_loss,\n",
    "                        'los_val': val_loss,\n",
    "                        'R2_val': val_R2,\n",
    "                        }, path_save)\n",
    "            if epoch%1000 == 999:\n",
    "                print(\"finished training 1st layer epoch\", epoch+1)\n",
    "                mtime=time.time()\n",
    "                print(\"train_loss: \", train_loss, \"train_R2\", train_R2,\"val_loss:\",val_loss,\"val_R2\", val_R2,\\\n",
    "                      \"loss val best:\",loss_val_best,\"R2 val best:\",R2_best, f\"Spending time: {mtime - starttime}s\")\n",
    "            if train_R2 > 0.99:\n",
    "                break\n",
    "            #adding early stop\n",
    "        model1.train()\n",
    "\n",
    "    path_fs = path_save+'fs'\n",
    "    torch.save({'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'model_state_dict_fs': model1.state_dict(),\n",
    "                }, path_fs)\n",
    "    #####finished training first layer\n",
    "    \n",
    "    ####################train the second layer, freeze first layer\n",
    "    \n",
    "    ###load the trained model:\n",
    "    n_a=64 #hidden state number\n",
    "    n_l=2 #layer of lstm\n",
    "    dropout=0.2\n",
    "    path_load = basic_path+model_version+'1l'\n",
    "    print(path_load)\n",
    "    checkpoint=torch.load(path_load)\n",
    "    model1=Statini_sq_N2OGRU(n_f+len(stat_values),n_f+len(stat_values),n_a,n_l,len(stat_values),1,dropout)\n",
    "    model1.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model1.to(device) #too large for GPU, kif not enough, change to cpu\n",
    "    print(model1)\n",
    "    loss_val_best = 500000\n",
    "    R2_best=0.5\n",
    "    path_save = basic_path+model_version+'2l'\n",
    "    \n",
    "    for name, param in model1.named_parameters():\n",
    "        param.requires_grad = True\n",
    "    ###freeze 1st layer\n",
    "    for name, param in model1.named_parameters():\n",
    "        if param.requires_grad and ('gru1' in name or 'densor1' in name):\n",
    "            param.requires_grad = False\n",
    "        if param.requires_grad:\n",
    "            print(name)\n",
    "\n",
    "    lr=0.1 #sgd\n",
    "    lr_adam=0.0001*0.5\n",
    "    \n",
    "    optimizer = optim.Adam(model1.parameters(), lr=lr_adam) #add weight decay normally 1-9e-4\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n",
    "    slw=122\n",
    "    print(X_train.size(),X_val.size())\n",
    "    batch_total=X_train.size(1)\n",
    "    batch_size=500  # this is the batch size for training\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    maxepoch=800\n",
    "    model1.train()\n",
    "    for epoch in range(maxepoch):\n",
    "        train_loss=0.0\n",
    "        val_loss=0.0\n",
    "        Y_pred_all=torch.zeros([Y_train.size(0),Y_train.size(1),len(out_names)],device=device)\n",
    "        #shuffled the training data\n",
    "        shuffled_b=torch.randperm(X_train.size()[1]) \n",
    "        X_train_new=X_train[:,shuffled_b,:] \n",
    "        Y_train_new=Y_train[:,shuffled_b,:]\n",
    "        Y_train_mask_new = Y_train_mask[:,shuffled_b,:]\n",
    "        Y_train_maskb_new = Y_train_maskb[:,shuffled_b,:]\n",
    "        stats_train_sq_new = stats_train_sq[:,shuffled_b,:]  \n",
    "        model1.zero_grad()\n",
    "        for bb in range(int(batch_total/batch_size)):\n",
    "            hidden = model1.init_hidden(batch_size)\n",
    "            \n",
    "            Y_pred,hidden = model1(X_train_new[:,bb*batch_size:(bb+1)*batch_size,:],\\\n",
    "                                   stats_train_sq_new[:,bb*batch_size:(bb+1)*batch_size,:],hidden)\n",
    "            #need to adjust the loss for missing data based on mask\n",
    "            loss = my_loss_weighted(Y_pred[:,:,pred_names_n[0]], \\\n",
    "                                    Y_train_new[:,bb*batch_size:(bb+1)*batch_size,Ynames_n[0]],\\\n",
    "                                    Y_train_mask_new[:,bb*batch_size:(bb+1)*batch_size,Ynames_n[0]])\n",
    "            hidden[0].detach_() \n",
    "            hidden[1].detach_()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                train_loss=train_loss+loss.item()\n",
    "                Y_pred_all[:,bb*batch_size:(bb+1)*batch_size,:]=Y_pred[:,:,:]\n",
    "       \n",
    "    \n",
    "        scheduler.step()\n",
    "        #validation\n",
    "        model1.eval()\n",
    "        with torch.no_grad():\n",
    "            train_loss=train_loss/(batch_total/batch_size)\n",
    "            train_losses.append(train_loss)\n",
    "            #mask out the points\n",
    "            Y_train_new_masked=torch.masked_select(Y_train_new[:,:,Ynames_n[0]], Y_train_maskb_new[:,:,Ynames_n[0]])\n",
    "            Y_pred_all_masked=torch.masked_select(Y_pred_all[:,:,pred_names_n[0]], Y_train_maskb_new[:,:,Ynames_n[0]])\n",
    "            train_R2=compute_r2(Y_pred_all_masked.contiguous().view(-1),\\\n",
    "                                Y_train_new_masked.contiguous().view(-1)).item()\n",
    "            ########################validation\n",
    "            Y_val_pred=torch.zeros([Y_val.size(0),Y_val.size(1),len(out_names)],device=device)\n",
    "            hidden = model1.init_hidden(X_val.size(1))\n",
    "            #print(stats_val_sq[:,2,0])\n",
    "            Y_val_pt, hidden = model1(X_val,stats_val_sq,hidden)\n",
    "            Y_val_pred[:,:,:] = Y_val_pt[:,:,:]\n",
    "            loss = my_loss_weighted(Y_val_pred[:,:,pred_names_n[0]],Y_val[:,:,Ynames_n[0]],Y_val_mask[:,:,Ynames_n[0]])\n",
    "            val_loss=loss.item()\n",
    "            val_losses.append(val_loss)\n",
    "            Y_val_masked=torch.masked_select(Y_val[:,:,Ynames_n[0]], Y_val_maskb[:,:,Ynames_n[0]])\n",
    "            Y_val_pred_masked=torch.masked_select(Y_val_pred[:,:,pred_names_n[0]], Y_val_maskb[:,:,Ynames_n[0]])\n",
    "            val_R2=compute_r2(Y_val_pred_masked.contiguous().view(-1),Y_val_masked.contiguous().view(-1)).item()\n",
    "            if val_loss < loss_val_best and val_R2 > R2_best:\n",
    "                loss_val_best=val_loss\n",
    "                R2_best = val_R2\n",
    "                f0=open(path_save,'w')\n",
    "                f0.close()\n",
    "                #os.remove(path_save)\n",
    "                torch.save({'epoch': epoch,\n",
    "                        'model_state_dict': model1.state_dict(),\n",
    "                        'R2': train_R2,\n",
    "                        'loss': train_loss,\n",
    "                        'los_val': val_loss,\n",
    "                        'R2_val': val_R2,\n",
    "                        }, path_save)\n",
    "            print(\"finished training epoch\", epoch+1)\n",
    "            mtime=time.time()\n",
    "            print(\"train_loss: \", train_loss, \"train_R2\", train_R2,\"val_loss:\",val_loss,\"val_R2\", val_R2,\\\n",
    "                  \"loss val best:\",loss_val_best,\"R2 val best:\",R2_best, f\"Spending time: {mtime - starttime}s\")\n",
    "            if train_R2 > 0.99:\n",
    "                break\n",
    "            #adding early stop\n",
    "        model1.train()\n",
    "    endtime=time.time()\n",
    "    path_fs = path_save+'fs'\n",
    "    torch.save({'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'model_state_dict_fs': model1.state_dict(),\n",
    "                }, path_fs)\n",
    "    print(\"final train_loss:\",train_loss,\"final train_R2:\",train_R2,\"val_loss:\",val_loss,\"loss validation best:\",loss_val_best)\n",
    "    print(f\"total Training time: {endtime - starttime}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
