





# Import Python libraries
import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

# Import classes/functions from the PyKGML
from time_series_models import GRUSeq2SeqWithAttention, SequenceDataset
from dataset import plot_features
import kgml_lib





# Load dataset
data_path = '/Users/yufengyang/Documents/GitHub/data/PyKGML/'
output_path = data_path + 'test_results/CO2/'
if not os.path.exists(output_path):
    os.makedirs(output_path)

pretrain_file = data_path + 'co2_pretrain_data.sav'
pretrain_data = torch.load(pretrain_file, weights_only=False)
finetune_file = data_path + 'co2_finetune_data.sav'
finetune_data = torch.load(finetune_file, weights_only=False)
print(pretrain_data.keys())
print(finetune_data.keys())


# Assign training and testing data. 
X_train, X_test, Y_train, Y_test = pretrain_data['X_train'], pretrain_data['X_test'], pretrain_data['Y_train'], pretrain_data['Y_test']
# y_scaler contains the means and stds. To reverse Y data to the original, use kgml.lib.Z_norm_reverse
x_scaler = pretrain_data['x_scaler']
y_scaler = pretrain_data['y_scaler']
input_features = pretrain_data['input_features']
output_features = pretrain_data['output_features']

print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)
print(x_scaler.shape, y_scaler.shape)
print(input_features, output_features)


# View histogram of input features
feature_len = len(input_features)
features = input_features
values = X_train.reshape(-1, feature_len)
sub_title = "Input features"
plot_features(values, feature_len, features, sub_title)


# View histogram of output features
feature_len = len(output_features)
features = output_features
values = Y_train.reshape(-1, feature_len)
sub_title = "Output features"
plot_features(values, feature_len, features, sub_title)





# Set parameters for a selected model
input_dim = len(input_features)
hidden_dim = 128
num_layers = 3
output_dim = len(output_features)
dropout = 0.2

# Define a model 
model = GRUSeq2SeqWithAttention(input_dim, hidden_dim, num_layers, output_dim, dropout)


# Use 365 consecutive days as a sample
sequence_length = 365  

# Create Dataset objects for training and testing.
train_dataset = SequenceDataset(X_train, Y_train, sequence_length)
test_dataset = SequenceDataset(X_test, Y_test, sequence_length)

# Create the dataLoaders for training and testing
batch_size = 64
model.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
model.test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False)


# Set hyperparameters for model training
learning_rate = 0.001
step_size = 20
max_epoch = 50
gamma = 0.6

# Set a loss function. Here we select nn.L1Loss() from PyTorch. nn.MSELoss() is another option 
loss_function = nn.L1Loss() 
# Set the path to save model weights
checkpoint_path = output_path + 'pretrained_GRU.pth'

# train the model
model.train_model(loss_func=loss_function, LR=learning_rate, step_size=step_size, gamma=gamma, maxepoch=max_epoch, checkpoint_path=checkpoint_path)


# model testing using test_dataset
model.test()





# Plot the learning curves
model.plot_training_curves()


# Calculate model's coefficient of determination
output_feature_name = ['Ra','Rh','NEE']
model.get_R2_score(y_scaler, output_feature_name)


# Visualize the time series of model prediction and true value at a given sample
model.vis_plot_prediction_result_time_series(y_scaler, output_feature_name, sample=60)


# Visulize the prediction vs. true value on a scatter plot
model.vis_scatter_prediction_result(y_scaler, output_feature_name)








# Import the help compiler for model structure design
ModelStructureCompiler = kgml_lib.ModelStructureCompiler

# Fill a configuration script to describe the proposed model structure
archt_config = {
    # define name and parameters
    'class_name': 'my_KGML',
    'base_class': 'TimeSeriesModel',
    'init_params': {
        'input_dim': 19,
        'hidden_dim': 128,
        'num_layers': 2,
        'output_dim': 3,
        'dropout': 0.2
    },
    # Define model components, where the key is a given name for a layer and the value is tuple of layer parameters. 
    # The first parameter is the layer name, which can be 'gru' for torch.nn.GRU or 'lstm' for torch.nn.LSTM.
    # The rest of the parameters in the tuple are parameters feed to the layer. 
    'layers': {
            'gru_basic': ('gru', 'input_dim', 'hidden_dim', 'num_layers', 'dropout'),
            'gru_ra':    ('gru', 'input_dim + hidden_dim', 'hidden_dim', 'num_layers', 'dropout'),
            'gru_rh':    ('gru', 'input_dim + hidden_dim', 'hidden_dim', 'num_layers', 'dropout'),
            'gru_nee':   ('gru', 'input_dim+2', 'hidden_dim', 'num_layers', 'dropout'),
            'dropout':   ('dropout', 'dropout'),
            'fc':        ('linear', 'hidden_dim', '1'),
    },
    # Define the forward function. This is where layers are connected to each other with inputs and outputs.
    'forward': {
        'out_basic, hidden': 'gru_basic(x)',
        'dropped':   'dropout(out_basic)',
        'ra_in':     'x & dropped',
        'ra_out, hidden':    'gru_ra(ra_in)',
        'ra_pred':   'fc(dropout(ra_out))',
        'rh_in':     'x & dropped',
        'rh_out, hidden':    'gru_rh(rh_in)',
        'rh_pred':   'fc(dropout(rh_out))',
        'nee_in':    'x & ra_pred & rh_pred',
        'nee_out, hidden':   'gru_nee(nee_in)',
        'nee_pred':  'fc(dropout(nee_out))',
        'output':    'ra_pred & rh_pred & nee_pred'
    }
}



# Feed the config to the pre-defined compiler. 
archt_compiler = ModelStructureCompiler(archt_config)

# Call generate_model() on the compiler to generate a model class.
KGML = archt_compiler.generate_model()

# Call class_code on the compiler to show the generated model structure.
print("Generated class code:")
print(archt_compiler.class_code)


# Define a model using the compiled architecture
myKGML1 = KGML()
# Create dataLoaders for the model
batch_size = 64
myKGML1.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
myKGML1.test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False)


# Set up hyperparameters
learning_rate = 0.001
step_size = 20
max_epoch = 50
gamma = 0.6
loss_function = nn.L1Loss() # nn.MSELoss()
checkpoint_path = output_path + 'customArcht_GRU.pth'

# Train the model
myKGML1.train_model(loss_func=loss_function, LR=learning_rate, step_size= step_size, gamma=gamma, maxepoch=max_epoch, checkpoint_path=checkpoint_path)


# Testing the model
myKGML1.test()

# visualization
myKGML1.plot_training_curves()

output_feature_name = ['Ra','Rh','NEE']
myKGML1.vis_scatter_prediction_result(y_scaler, output_feature_name)





# Import the help compiler
LossFunctionCompiler = kgml_lib.LossFunctionCompiler

# GPP is the No. 9 feature in the inputs X
GPP_scaler = x_scaler[8]
# configure a script for building a loss function
lossfn_config = {
    'parameters': {
        'GPP_idx': 8,
        'Ra_idx': 0,
        'Rh_idx': 1,
        'NEE_idx': 2,
        'tol_MB': 0.01,
        'aaa_scaler': x_scaler,
        'ccc_scaler':y_scaler},
    # Explicitly define each term that will be used in the loss function.
    # Because X and Y had been normalized, terms in the mass balance need to be reversed.
    'variables': {
        'Ra_pred': 'y_pred[:, :, Ra_idx]',
        'Rh_pred': 'y_pred[:, :, Rh_idx]',
        'NEE_pred': 'y_pred[:, :, NEE_idx]',
        'GPP_reverse': 'Z_norm_reverse(batch_x[:, :, GPP_idx], aaa_scaler[GPP_idx])',
        'Ra_pred_reverse': 'Z_norm_reverse(y_pred[:, :, Ra_idx], ccc_scaler[Ra_idx])',
        'Rh_pred_reverse': 'Z_norm_reverse(y_pred[:, :, Rh_idx], ccc_scaler[Rh_idx])',
        'NEE_pred_reverse': 'Z_norm_reverse(y_pred[:, :, NEE_idx], ccc_scaler[NEE_idx])',
        'Ra_true': 'y_true[:, :, Ra_idx]',
        'Rh_true': 'y_true[:, :, Rh_idx]',
        'NEE_true': 'y_true[:, :, NEE_idx]',
        },
        # Define the loss function.
    'loss_formula': {
        'Reco_pred_reverse': 'Ra_pred_reverse + Rh_pred_reverse',
        'loss1': 'mean((Ra_pred - Ra_true)**2 + (Rh_pred - Rh_true)**2 + (NEE_pred - NEE_true)**2)',
        'loss2': 'mean(relu(abs(GPP_reverse + Reco_pred_reverse + NEE_pred_reverse)  - tol_MB * abs(Reco_pred_reverse)))',
        'loss': 'loss1 + loss2',}
    }


# Create the compiler
lossfn_compiler = LossFunctionCompiler(lossfn_config)

# Print the created loss class code
print("Generated CarbonFluxLoss:")
print(lossfn_compiler.class_code)

# Create the loss class
customLoss = lossfn_compiler.generate_class()
loss_fn = customLoss()


# We will create a model the same as the model in step 3 to use the loss function
myKGML2 = GRUSeq2SeqWithAttention(input_dim, hidden_dim, num_layers, output_dim, dropout)

# Load the model weights from the training of the previous model
checkpoint_path = output_path + 'pretrained_GRU.pth'
checkpoint = torch.load(checkpoint_path, weights_only=True)
myKGML2.load_pretrained(pretrained_model_path=checkpoint_path)

# Create DataLoaders
myKGML2.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
myKGML2.test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False)





batch_size = 64
learning_rate = 0.001
step_size = 10
max_epoch = 25
gamma = 0.6

new_checkpoint_path = output_path + 'customLoss_GRU.pth'
# Train the model with the customzed loss function
myKGML2.train_model(loss_func=loss_fn, LR=learning_rate, step_size=step_size, gamma=gamma, maxepoch=max_epoch, checkpoint_path=new_checkpoint_path)



# Test the model
myKGML2.test()

# Visualize the learning curves
myKGML2.plot_training_curves()

# Visualize a scatter plot
output_feature_name = ['Ra','Rh','NEE']
myKGML2.vis_scatter_prediction_result(y_scaler, output_feature_name)





# Assign training and testing data. 
X_train_ft, X_test_ft, Y_train_ft, Y_test_ft = finetune_data['X_train'], finetune_data['X_test'], finetune_data['Y_train'], finetune_data['Y_test']
x_scaler_ft = finetune_data['x_scaler']
input_features_ft = finetune_data['input_features']
output_features_ft = finetune_data['output_features']

print(X_train_ft.shape, Y_train_ft.shape, X_test_ft.shape, Y_test_ft.shape)
print(input_features_ft,output_features_ft)


# configure a script to customize a loss function for fune-tuning

ft_lossfn_config = {
    'parameters': {
        'GPP_idx': 8,
        'Ra_pred_idx': 0,
        'Rh_pred_idx': 1,
        'NEE_pred_idx': 2,
        'NEE_true_idx': 0,
        'Reco_true_idx': 1,
        'aaa_scaler': x_scaler,
        'bbb_scaler': y_scaler},
    # revise this part to remove scaler and z_norm_reverse. 1. scaling methods (pre-define) 2. scaler
    'variables': {
        'Ra_pred': 'y_pred[:, :, Ra_pred_idx]',
        'Rh_pred': 'y_pred[:, :, Rh_pred_idx]',
        'NEE_pred': 'y_pred[:, :, NEE_pred_idx]',
        'NEE_true': 'y_true[:, :, NEE_true_idx]',
        'Reco_true': 'y_true[:, :, Reco_true_idx]',
        'GPP_reverse': 'Z_norm_reverse(batch_x[:, :, GPP_idx], aaa_scaler[GPP_idx])',
        'Ra_pred_reverse': '-Z_norm_reverse(y_pred[:, :, Ra_pred_idx], bbb_scaler[Ra_pred_idx])',
        'Rh_pred_reverse': '-Z_norm_reverse(y_pred[:, :, Rh_pred_idx], bbb_scaler[Rh_pred_idx])',
        'NEE_pred_reverse': 'Z_norm_reverse(y_pred[:, :, NEE_pred_idx], bbb_scaler[NEE_pred_idx])',
        },
    
    'loss_formula': {
        'Reco_pred_reverse': 'Ra_pred_reverse + Rh_pred_reverse',
        'loss': 'mean((NEE_pred_reverse - NEE_true)**2 + (Reco_pred_reverse - Reco_true)**2)',
        }
    }


# Create the compiler
ft_lossfn_compiler = LossFunctionCompiler(ft_lossfn_config)

# Print the created loss class code
print("Generated CarbonFluxLoss:")
print(ft_lossfn_compiler.class_code)

# Create the loss class
ft_customLoss = ft_lossfn_compiler.generate_class()
ft_loss_fn = ft_customLoss()


# Define hyperparameters
ft_input_dim = len(input_features)
hidden_dim = 128
num_layers = 3
ft_output_dim = len(output_features)
dropout=0.2

# Locate the weight file saved during the pretraining 
checkpoint_path = output_path + 'pretrained_GRU.pth'
checkpoint = torch.load(checkpoint_path, weights_only=True)

# Define the fine-tuning model parameters, which be the same as the pretrained model
# KGML_ft = KGML(input_dim, hidden_dim, num_layers, ft_output_dim, dropout)
KGML_ft = GRUSeq2SeqWithAttention(ft_input_dim, hidden_dim, num_layers, ft_output_dim, dropout)
# reload model weights
KGML_ft.load_pretrained(pretrained_model_path=checkpoint_path)


# Use 365 consecutive days as a sample
sequence_length = 365

# Create Dataset objects for training and testing.
train_dataset_ft = SequenceDataset(X_train_ft, Y_train_ft, sequence_length)
test_dataset_ft = SequenceDataset(X_test_ft, Y_test_ft, sequence_length)

# Create DataLoaders.
batch_size = 64
KGML_ft.train_loader = DataLoader(train_dataset_ft, batch_size=batch_size, shuffle=True)
KGML_ft.test_loader  = DataLoader(test_dataset_ft, batch_size=1, shuffle=False)


learning_rate = 0.001
step_size = 20
max_epoch = 100
gamma = 0.8

ft_checkpoint_path = output_path + 'finetune_GRU.pth'
# Train the model
KGML_ft.train_model(loss_func=ft_loss_fn, LR=learning_rate, step_size=step_size, gamma=gamma, maxepoch=max_epoch, checkpoint_path=ft_checkpoint_path)





# Import functions for calculating R2 and visualizing results
from kgml_lib import get_R2_score, vis_scatter_prediction_result, Z_norm_reverse
KGML_ft.test()
# Visualize the learning curves
KGML_ft.plot_training_curves()

# Construct a prediction that contains [NEE, Reco] from the direct model outputs of [Ra, Rh, NEE]
ft_Ra_pred = Z_norm_reverse(KGML_ft.all_predictions[:,:,0], y_scaler[0])
ft_Rh_pred = Z_norm_reverse(KGML_ft.all_predictions[:,:,1], y_scaler[1])
ft_NEE_pred = Z_norm_reverse(KGML_ft.all_predictions[:,:,2], y_scaler[2])
ft_predictions = torch.stack([ft_NEE_pred, -(ft_Ra_pred + ft_Rh_pred)], dim=2)
ft_targets = KGML_ft.all_targets

# The predictions have been reversed using the y_scaler.
y_scaler_ft = None
output_feature_name = ['NEE', 'Reco']
vis_scatter_prediction_result(target=ft_targets, prediction=ft_predictions, y_scaler=y_scaler_ft, features=output_feature_name)



